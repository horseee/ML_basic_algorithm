{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary序列的规则\n",
    "输入数据X：在时间t，$X_t$的值有50%的概率为1，50%的概率为0；  \n",
    "输出数据Y：在时间t，$Y_t$的值有50%的概率为1，50%的概率为0，除此之外，还有两条规则：\n",
    "* 规则1:如果$X_{t-3}$ == 1，$Y_t$为1的概率增加50%\n",
    "* 规则2:如果$X_{t-8}$ == 1，则$Y_t$为1的概率减少25%， 如果上述两个条件同时满足，则$Y_t$为1的概率为75%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(size = 1000000, second_rule_idx = 8):\n",
    "    X = np.array(np.random.choice(2, size=(size, )))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pro_list):\n",
    "    sum = np.sum(-pro * np.log(pro) for pro in pro_list)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If not learning any rule, the cross entropy = 0.661563\n",
      "If learning rule one, the cross entropy = 0.519167\n",
      "If learning all rule, the cross entropy = 0.454454\n"
     ]
    }
   ],
   "source": [
    "Entropy_No_Train = cross_entropy([0.625, 1 - 0.625])\n",
    "print(\"If not learning any rule, the cross entropy = %4f\" % (Entropy_No_Train))\n",
    "Entropy_Train_Rule_One = cross_entropy([0.875, 0.125]) * 0.5 + cross_entropy([0.625, 0.375]) * 0.5\n",
    "print(\"If learning rule one, the cross entropy = %4f\" % (Entropy_Train_Rule_One))\n",
    "Entropy_Train_All_Rule = cross_entropy([0.75, 0.25]) * 0.5 + cross_entropy([0.5, 0.5]) * 0.25\n",
    "print(\"If learning all rule, the cross entropy = %4f\" % (Entropy_Train_All_Rule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(raw, batch_size, num_steps):\n",
    "    X, Y = raw\n",
    "    X_length = len(X)\n",
    "    \n",
    "    batch_partition_length = X_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype = np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype = np.int32) \n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = X[batch_partition_length * i : batch_partition_length * (i+1)]\n",
    "        data_y[i] = Y[batch_partition_length * i : batch_partition_length * (i+1)]\n",
    "\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, num_steps * i: num_steps * (i+1)]\n",
    "        y = data_y[:, num_steps * i: num_steps * (i+1)]\n",
    "        yield(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, second_idx = 8):\n",
    "    print(second_idx)\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(second_rule_idx = second_idx), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.002\n",
    "num_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='output_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.zeros([batch_size, state_size])\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rnn cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer = tf.constant_initializer(0.0))\n",
    "    \n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse = True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer = tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_state\n",
    "rnn_outputs = []\n",
    "\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer = tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unstack = tf.unstack(y, axis=1)\n",
    "losses =  [tf.nn.sparse_softmax_cross_entropy_with_logits(labels = label, logits = logit) \\\n",
    "    for logit, label in zip(logits, y_unstack)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(num_epochs, verbose = True, second_idx = 8):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        \n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, second_idx)):\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            training_loss = 0\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            \n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                \n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run( [losses,total_loss,final_state,train_step],\n",
    "                              feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                \n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step \", step, \" for last 100 steps:\", training_loss / 100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "    \n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\n",
      "EPOCH 0\n",
      "Average loss at step  100  for last 100 steps: 0.632960320115\n",
      "Average loss at step  200  for last 100 steps: 0.545460736454\n",
      "Average loss at step  300  for last 100 steps: 0.535406336486\n",
      "Average loss at step  400  for last 100 steps: 0.529344138801\n",
      "Average loss at step  500  for last 100 steps: 0.518623972833\n",
      "Average loss at step  600  for last 100 steps: 0.530396513939\n",
      "Average loss at step  700  for last 100 steps: 0.534039851725\n",
      "Average loss at step  800  for last 100 steps: 0.528763493896\n",
      "Average loss at step  900  for last 100 steps: 0.522785362005\n",
      "Average loss at step  1000  for last 100 steps: 0.520684952438\n",
      "Average loss at step  1100  for last 100 steps: 0.522965523899\n",
      "Average loss at step  1200  for last 100 steps: 0.508750708699\n",
      "Average loss at step  1300  for last 100 steps: 0.518433773518\n",
      "Average loss at step  1400  for last 100 steps: 0.516557570398\n",
      "Average loss at step  1500  for last 100 steps: 0.516608287394\n",
      "Average loss at step  1600  for last 100 steps: 0.526686725318\n",
      "Average loss at step  1700  for last 100 steps: 0.530220012069\n",
      "Average loss at step  1800  for last 100 steps: 0.523577949405\n",
      "Average loss at step  1900  for last 100 steps: 0.517983013988\n",
      "Average loss at step  2000  for last 100 steps: 0.529134268165\n",
      "Average loss at step  2100  for last 100 steps: 0.51974022001\n",
      "Average loss at step  2200  for last 100 steps: 0.509247521162\n",
      "Average loss at step  2300  for last 100 steps: 0.519553025365\n",
      "Average loss at step  2400  for last 100 steps: 0.522119841278\n",
      "Average loss at step  2500  for last 100 steps: 0.504334048033\n",
      "Average loss at step  2600  for last 100 steps: 0.520590641499\n",
      "Average loss at step  2700  for last 100 steps: 0.509708210826\n",
      "Average loss at step  2800  for last 100 steps: 0.508431270421\n",
      "Average loss at step  2900  for last 100 steps: 0.511002274752\n",
      "Average loss at step  3000  for last 100 steps: 0.529813889861\n",
      "Average loss at step  3100  for last 100 steps: 0.510566965342\n",
      "Average loss at step  3200  for last 100 steps: 0.518946588337\n",
      "Average loss at step  3300  for last 100 steps: 0.513366978467\n",
      "Average loss at step  3400  for last 100 steps: 0.541519811153\n",
      "Average loss at step  3500  for last 100 steps: 0.532242789865\n",
      "Average loss at step  3600  for last 100 steps: 0.508838113844\n",
      "Average loss at step  3700  for last 100 steps: 0.535505750775\n",
      "Average loss at step  3800  for last 100 steps: 0.525606385767\n",
      "Average loss at step  3900  for last 100 steps: 0.517425460815\n",
      "Average loss at step  4000  for last 100 steps: 0.520930457711\n",
      "Average loss at step  4100  for last 100 steps: 0.521996772885\n",
      "Average loss at step  4200  for last 100 steps: 0.522628190517\n",
      "Average loss at step  4300  for last 100 steps: 0.523098761439\n",
      "Average loss at step  4400  for last 100 steps: 0.534761435091\n",
      "Average loss at step  4500  for last 100 steps: 0.51926843226\n",
      "Average loss at step  4600  for last 100 steps: 0.529093553722\n",
      "Average loss at step  4700  for last 100 steps: 0.533511918187\n",
      "Average loss at step  4800  for last 100 steps: 0.512725556493\n",
      "Average loss at step  4900  for last 100 steps: 0.499357863069\n",
      "Average loss at step  5000  for last 100 steps: 0.51580748111\n",
      "Average loss at step  5100  for last 100 steps: 0.523536372185\n",
      "Average loss at step  5200  for last 100 steps: 0.524797946811\n",
      "Average loss at step  5300  for last 100 steps: 0.521935712695\n",
      "Average loss at step  5400  for last 100 steps: 0.518015475869\n",
      "Average loss at step  5500  for last 100 steps: 0.520113938451\n",
      "Average loss at step  5600  for last 100 steps: 0.540291242301\n",
      "Average loss at step  5700  for last 100 steps: 0.514495005012\n",
      "Average loss at step  5800  for last 100 steps: 0.52025080651\n",
      "Average loss at step  5900  for last 100 steps: 0.527623271942\n",
      "Average loss at step  6000  for last 100 steps: 0.528211978078\n",
      "Average loss at step  6100  for last 100 steps: 0.533788890839\n",
      "Average loss at step  6200  for last 100 steps: 0.53682797879\n",
      "Average loss at step  6300  for last 100 steps: 0.526291396618\n",
      "Average loss at step  6400  for last 100 steps: 0.506458537579\n",
      "Average loss at step  6500  for last 100 steps: 0.524730402529\n",
      "Average loss at step  6600  for last 100 steps: 0.51431856513\n",
      "Average loss at step  6700  for last 100 steps: 0.494588711262\n",
      "Average loss at step  6800  for last 100 steps: 0.51536676228\n",
      "Average loss at step  6900  for last 100 steps: 0.537392122447\n",
      "Average loss at step  7000  for last 100 steps: 0.513434666991\n",
      "Average loss at step  7100  for last 100 steps: 0.520393922627\n",
      "Average loss at step  7200  for last 100 steps: 0.522030678689\n",
      "Average loss at step  7300  for last 100 steps: 0.526530367136\n",
      "Average loss at step  7400  for last 100 steps: 0.517705850899\n",
      "Average loss at step  7500  for last 100 steps: 0.515168302059\n",
      "Average loss at step  7600  for last 100 steps: 0.508407419026\n",
      "Average loss at step  7700  for last 100 steps: 0.523655730784\n",
      "Average loss at step  7800  for last 100 steps: 0.518756614029\n",
      "Average loss at step  7900  for last 100 steps: 0.526239133477\n",
      "Average loss at step  8000  for last 100 steps: 0.527386119068\n",
      "Average loss at step  8100  for last 100 steps: 0.524987927973\n",
      "Average loss at step  8200  for last 100 steps: 0.512424313426\n",
      "Average loss at step  8300  for last 100 steps: 0.517878704667\n",
      "Average loss at step  8400  for last 100 steps: 0.523937521875\n",
      "Average loss at step  8500  for last 100 steps: 0.512627806664\n",
      "Average loss at step  8600  for last 100 steps: 0.500777454078\n",
      "Average loss at step  8700  for last 100 steps: 0.504014437199\n",
      "Average loss at step  8800  for last 100 steps: 0.51218270272\n",
      "Average loss at step  8900  for last 100 steps: 0.509724625647\n",
      "Average loss at step  9000  for last 100 steps: 0.528702547252\n",
      "Average loss at step  9100  for last 100 steps: 0.530014294982\n",
      "Average loss at step  9200  for last 100 steps: 0.521344921887\n",
      "Average loss at step  9300  for last 100 steps: 0.517136543989\n",
      "Average loss at step  9400  for last 100 steps: 0.501420823932\n",
      "Average loss at step  9500  for last 100 steps: 0.536347305775\n",
      "Average loss at step  9600  for last 100 steps: 0.514639196396\n",
      "Average loss at step  9700  for last 100 steps: 0.526357088685\n",
      "Average loss at step  9800  for last 100 steps: 0.519022511244\n",
      "Average loss at step  9900  for last 100 steps: 0.518516331315\n",
      "Average loss at step  10000  for last 100 steps: 0.509025800228\n",
      "Average loss at step  10100  for last 100 steps: 0.525240819156\n",
      "Average loss at step  10200  for last 100 steps: 0.52464356035\n",
      "Average loss at step  10300  for last 100 steps: 0.514277831912\n",
      "Average loss at step  10400  for last 100 steps: 0.523624451458\n",
      "Average loss at step  10500  for last 100 steps: 0.524653410017\n",
      "Average loss at step  10600  for last 100 steps: 0.514376344085\n",
      "Average loss at step  10700  for last 100 steps: 0.520975261033\n",
      "Average loss at step  10800  for last 100 steps: 0.5289354375\n",
      "Average loss at step  10900  for last 100 steps: 0.521602567136\n",
      "Average loss at step  11000  for last 100 steps: 0.507317069173\n",
      "Average loss at step  11100  for last 100 steps: 0.50947508812\n",
      "Average loss at step  11200  for last 100 steps: 0.517283956409\n",
      "Average loss at step  11300  for last 100 steps: 0.520305612981\n",
      "Average loss at step  11400  for last 100 steps: 0.523912327588\n",
      "Average loss at step  11500  for last 100 steps: 0.512726703584\n",
      "Average loss at step  11600  for last 100 steps: 0.516816096306\n",
      "Average loss at step  11700  for last 100 steps: 0.523305907249\n",
      "Average loss at step  11800  for last 100 steps: 0.533211627603\n",
      "Average loss at step  11900  for last 100 steps: 0.517873802185\n",
      "Average loss at step  12000  for last 100 steps: 0.519487343431\n",
      "Average loss at step  12100  for last 100 steps: 0.499637558758\n",
      "Average loss at step  12200  for last 100 steps: 0.493237693012\n",
      "Average loss at step  12300  for last 100 steps: 0.51385528177\n",
      "Average loss at step  12400  for last 100 steps: 0.510353200138\n",
      "Average loss at step  12500  for last 100 steps: 0.505149815083\n",
      "Average loss at step  12600  for last 100 steps: 0.51155184567\n",
      "Average loss at step  12700  for last 100 steps: 0.529293092787\n",
      "Average loss at step  12800  for last 100 steps: 0.499560294747\n",
      "Average loss at step  12900  for last 100 steps: 0.500569245815\n",
      "Average loss at step  13000  for last 100 steps: 0.51057458967\n",
      "Average loss at step  13100  for last 100 steps: 0.52965277791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  13200  for last 100 steps: 0.518720411956\n",
      "Average loss at step  13300  for last 100 steps: 0.508552961946\n",
      "Average loss at step  13400  for last 100 steps: 0.529070930183\n",
      "Average loss at step  13500  for last 100 steps: 0.523540932834\n",
      "Average loss at step  13600  for last 100 steps: 0.509052830935\n",
      "Average loss at step  13700  for last 100 steps: 0.519270059168\n",
      "Average loss at step  13800  for last 100 steps: 0.524004679918\n",
      "Average loss at step  13900  for last 100 steps: 0.50911965549\n",
      "Average loss at step  14000  for last 100 steps: 0.521075017154\n",
      "Average loss at step  14100  for last 100 steps: 0.514835495353\n",
      "Average loss at step  14200  for last 100 steps: 0.49822563529\n",
      "Average loss at step  14300  for last 100 steps: 0.511143222451\n",
      "Average loss at step  14400  for last 100 steps: 0.510696793795\n",
      "Average loss at step  14500  for last 100 steps: 0.521394702196\n",
      "Average loss at step  14600  for last 100 steps: 0.52416745156\n",
      "Average loss at step  14700  for last 100 steps: 0.51558382839\n",
      "Average loss at step  14800  for last 100 steps: 0.497465111315\n",
      "Average loss at step  14900  for last 100 steps: 0.508696473837\n",
      "Average loss at step  15000  for last 100 steps: 0.506732791662\n",
      "Average loss at step  15100  for last 100 steps: 0.504410986304\n",
      "Average loss at step  15200  for last 100 steps: 0.503520905972\n",
      "Average loss at step  15300  for last 100 steps: 0.524025133252\n",
      "Average loss at step  15400  for last 100 steps: 0.51102735281\n",
      "Average loss at step  15500  for last 100 steps: 0.519222893417\n",
      "Average loss at step  15600  for last 100 steps: 0.521198540628\n",
      "Average loss at step  15700  for last 100 steps: 0.519646825194\n",
      "Average loss at step  15800  for last 100 steps: 0.514100800753\n",
      "Average loss at step  15900  for last 100 steps: 0.517701456249\n",
      "Average loss at step  16000  for last 100 steps: 0.520144578218\n",
      "Average loss at step  16100  for last 100 steps: 0.514131040275\n",
      "Average loss at step  16200  for last 100 steps: 0.494111738205\n",
      "Average loss at step  16300  for last 100 steps: 0.514553926289\n",
      "Average loss at step  16400  for last 100 steps: 0.514862630367\n",
      "Average loss at step  16500  for last 100 steps: 0.510898530781\n",
      "Average loss at step  16600  for last 100 steps: 0.5053506428\n",
      "Average loss at step  16700  for last 100 steps: 0.507361166775\n",
      "Average loss at step  16800  for last 100 steps: 0.504504408836\n",
      "Average loss at step  16900  for last 100 steps: 0.519107869565\n",
      "Average loss at step  17000  for last 100 steps: 0.519019538462\n",
      "Average loss at step  17100  for last 100 steps: 0.510271078348\n",
      "Average loss at step  17200  for last 100 steps: 0.520310191214\n",
      "Average loss at step  17300  for last 100 steps: 0.52926371932\n",
      "Average loss at step  17400  for last 100 steps: 0.520081365108\n",
      "Average loss at step  17500  for last 100 steps: 0.516156073809\n",
      "Average loss at step  17600  for last 100 steps: 0.51442050904\n",
      "Average loss at step  17700  for last 100 steps: 0.506984296441\n",
      "Average loss at step  17800  for last 100 steps: 0.516339299679\n",
      "Average loss at step  17900  for last 100 steps: 0.509150180817\n",
      "Average loss at step  18000  for last 100 steps: 0.51390722096\n",
      "Average loss at step  18100  for last 100 steps: 0.519247215092\n",
      "Average loss at step  18200  for last 100 steps: 0.506856224537\n",
      "Average loss at step  18300  for last 100 steps: 0.514823193252\n",
      "Average loss at step  18400  for last 100 steps: 0.506501882672\n",
      "Average loss at step  18500  for last 100 steps: 0.509569664001\n",
      "Average loss at step  18600  for last 100 steps: 0.505186164081\n",
      "Average loss at step  18700  for last 100 steps: 0.516502152085\n",
      "Average loss at step  18800  for last 100 steps: 0.508403325975\n",
      "Average loss at step  18900  for last 100 steps: 0.528239617646\n",
      "Average loss at step  19000  for last 100 steps: 0.511829434335\n",
      "Average loss at step  19100  for last 100 steps: 0.519600223005\n",
      "Average loss at step  19200  for last 100 steps: 0.522440210879\n",
      "Average loss at step  19300  for last 100 steps: 0.523403062224\n",
      "Average loss at step  19400  for last 100 steps: 0.507532238364\n",
      "Average loss at step  19500  for last 100 steps: 0.513039936423\n",
      "Average loss at step  19600  for last 100 steps: 0.503880193532\n",
      "Average loss at step  19700  for last 100 steps: 0.512700099349\n",
      "Average loss at step  19800  for last 100 steps: 0.530244008899\n",
      "Average loss at step  19900  for last 100 steps: 0.514401438832\n",
      "Average loss at step  20000  for last 100 steps: 0.5085712713\n",
      "Average loss at step  20100  for last 100 steps: 0.516784534752\n",
      "Average loss at step  20200  for last 100 steps: 0.511478981078\n",
      "Average loss at step  20300  for last 100 steps: 0.512383643687\n",
      "Average loss at step  20400  for last 100 steps: 0.532552544177\n",
      "Average loss at step  20500  for last 100 steps: 0.519467403591\n",
      "Average loss at step  20600  for last 100 steps: 0.528388912678\n",
      "Average loss at step  20700  for last 100 steps: 0.533691366911\n",
      "Average loss at step  20800  for last 100 steps: 0.514020493031\n",
      "Average loss at step  20900  for last 100 steps: 0.519955179989\n",
      "Average loss at step  21000  for last 100 steps: 0.522371122837\n",
      "Average loss at step  21100  for last 100 steps: 0.51890555948\n",
      "Average loss at step  21200  for last 100 steps: 0.511665261686\n",
      "Average loss at step  21300  for last 100 steps: 0.51589733541\n",
      "Average loss at step  21400  for last 100 steps: 0.518301259279\n",
      "Average loss at step  21500  for last 100 steps: 0.518023279905\n",
      "Average loss at step  21600  for last 100 steps: 0.51225479722\n",
      "Average loss at step  21700  for last 100 steps: 0.542000158727\n",
      "Average loss at step  21800  for last 100 steps: 0.515497047603\n",
      "Average loss at step  21900  for last 100 steps: 0.522526055276\n",
      "Average loss at step  22000  for last 100 steps: 0.510992614031\n",
      "Average loss at step  22100  for last 100 steps: 0.519292419255\n",
      "Average loss at step  22200  for last 100 steps: 0.5017999807\n",
      "Average loss at step  22300  for last 100 steps: 0.519801027179\n",
      "Average loss at step  22400  for last 100 steps: 0.512848276198\n",
      "Average loss at step  22500  for last 100 steps: 0.522753657401\n",
      "Average loss at step  22600  for last 100 steps: 0.517739881277\n",
      "Average loss at step  22700  for last 100 steps: 0.515616929233\n",
      "Average loss at step  22800  for last 100 steps: 0.521459111869\n",
      "Average loss at step  22900  for last 100 steps: 0.502237349153\n",
      "Average loss at step  23000  for last 100 steps: 0.513933132887\n",
      "Average loss at step  23100  for last 100 steps: 0.523553695083\n",
      "Average loss at step  23200  for last 100 steps: 0.499844096601\n",
      "Average loss at step  23300  for last 100 steps: 0.515344611704\n",
      "Average loss at step  23400  for last 100 steps: 0.528096633554\n",
      "Average loss at step  23500  for last 100 steps: 0.520292808712\n",
      "Average loss at step  23600  for last 100 steps: 0.521699460745\n",
      "Average loss at step  23700  for last 100 steps: 0.527402041554\n",
      "Average loss at step  23800  for last 100 steps: 0.509090672433\n",
      "Average loss at step  23900  for last 100 steps: 0.51643089354\n",
      "Average loss at step  24000  for last 100 steps: 0.516600820422\n",
      "Average loss at step  24100  for last 100 steps: 0.513405752182\n",
      "Average loss at step  24200  for last 100 steps: 0.512262393832\n",
      "Average loss at step  24300  for last 100 steps: 0.509063295722\n",
      "Average loss at step  24400  for last 100 steps: 0.515094712079\n",
      "Average loss at step  24500  for last 100 steps: 0.526892942488\n",
      "Average loss at step  24600  for last 100 steps: 0.502855710685\n",
      "Average loss at step  24700  for last 100 steps: 0.514061952233\n",
      "Average loss at step  24800  for last 100 steps: 0.515602893829\n",
      "Average loss at step  24900  for last 100 steps: 0.513842991889\n",
      "Average loss at step  25000  for last 100 steps: 0.518958214223\n",
      "Average loss at step  25100  for last 100 steps: 0.52828707546\n",
      "Average loss at step  25200  for last 100 steps: 0.515131656826\n",
      "Average loss at step  25300  for last 100 steps: 0.498509225249\n",
      "Average loss at step  25400  for last 100 steps: 0.527915556133\n",
      "Average loss at step  25500  for last 100 steps: 0.515938692689\n",
      "Average loss at step  25600  for last 100 steps: 0.518259675801\n",
      "Average loss at step  25700  for last 100 steps: 0.52410749495\n",
      "Average loss at step  25800  for last 100 steps: 0.516346154511\n",
      "Average loss at step  25900  for last 100 steps: 0.514312475622\n",
      "Average loss at step  26000  for last 100 steps: 0.511408143342\n",
      "Average loss at step  26100  for last 100 steps: 0.536770994067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  26200  for last 100 steps: 0.515116798878\n",
      "Average loss at step  26300  for last 100 steps: 0.526327954233\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(1, True, 8)\n",
    "plt.plot(training_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -8 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-a4e6e506576e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-837687f797af>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(num_epochs, verbose, second_idx)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtraining_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mtraining_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-18f69033ea6f>\u001b[0m in \u001b[0;36mgen_epochs\u001b[0;34m(n, num_steps, second_idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgen_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mgen_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-eaa8b2ce09a5>\u001b[0m in \u001b[0;36mgen_data\u001b[0;34m(size, second_rule_idx)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mthreshold\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msecond_rule_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mthreshold\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -8 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(1, True, 5)\n",
    "plt.plot(training_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
