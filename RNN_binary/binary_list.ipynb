{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary序列的规则\n",
    "输入数据X：在时间t，$X_t$的值有50%的概率为1，50%的概率为0；  \n",
    "输出数据Y：在时间t，$Y_t$的值有50%的概率为1，50%的概率为0，除此之外，还有两条规则：\n",
    "* 规则1:如果$X_{t-3}$ == 1，$Y_t$为1的概率增加50%\n",
    "* 规则2:如果$X_{t-8}$ == 1，则$Y_t$为1的概率减少25%， 如果上述两个条件同时满足，则$Y_t$为1的概率为75%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(size = 1000000, second_rule_idx = 8):\n",
    "    X = np.array(np.random.choice(2, size=(size, )))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pro_list):\n",
    "    sum = np.sum(-pro * np.log(pro) for pro in pro_list)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If not learning any rule, the cross entropy = 0.661563\n",
      "If learning rule one, the cross entropy = 0.519167\n",
      "If learning all rule, the cross entropy = 0.454454\n"
     ]
    }
   ],
   "source": [
    "Entropy_No_Train = cross_entropy([0.625, 1 - 0.625])\n",
    "print(\"If not learning any rule, the cross entropy = %4f\" % (Entropy_No_Train))\n",
    "Entropy_Train_Rule_One = cross_entropy([0.875, 0.125]) * 0.5 + cross_entropy([0.625, 0.375]) * 0.5\n",
    "print(\"If learning rule one, the cross entropy = %4f\" % (Entropy_Train_Rule_One))\n",
    "Entropy_Train_All_Rule = cross_entropy([0.75, 0.25]) * 0.5 + cross_entropy([0.5, 0.5]) * 0.25\n",
    "print(\"If learning all rule, the cross entropy = %4f\" % (Entropy_Train_All_Rule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(raw, batch_size, num_steps):\n",
    "    X, Y = raw\n",
    "    X_length = len(X)\n",
    "    \n",
    "    batch_partition_length = X_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype = np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype = np.int32) \n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = X[batch_partition_length * i : batch_partition_length * (i+1)]\n",
    "        data_y[i] = Y[batch_partition_length * i : batch_partition_length * (i+1)]\n",
    "\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, num_steps * i: num_steps * (i+1)]\n",
    "        y = data_y[:, num_steps * i: num_steps * (i+1)]\n",
    "        yield(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps, second_idx = 8):\n",
    "    print(second_idx)\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(second_rule_idx = second_idx), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "num_classes = 2\n",
    "state_size = 16\n",
    "learning_rate = 0.002\n",
    "num_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='output_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.zeros([batch_size, state_size])\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rnn cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer = tf.constant_initializer(0.0))\n",
    "    \n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse = True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer = tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = init_state\n",
    "rnn_outputs = []\n",
    "\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer = tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_unstack = tf.unstack(y, axis=1)\n",
    "losses =  [tf.nn.sparse_softmax_cross_entropy_with_logits(labels = label, logits = logit) \\\n",
    "    for logit, label in zip(logits, y_unstack)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(num_epochs, verbose = True, second_idx = 8):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        \n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, second_idx)):\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            training_loss = 0\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            \n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                \n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run( [losses,total_loss,final_state,train_step],\n",
    "                              feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                \n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step \", step, \" for last 100 steps:\", training_loss / 100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "    \n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "\n",
      "EPOCH 0\n",
      "Average loss at step  100  for last 100 steps: 0.742272976637\n",
      "Average loss at step  200  for last 100 steps: 0.620792517066\n",
      "Average loss at step  300  for last 100 steps: 0.578356666267\n",
      "Average loss at step  400  for last 100 steps: 0.570386106074\n",
      "Average loss at step  500  for last 100 steps: 0.552660898864\n",
      "Average loss at step  600  for last 100 steps: 0.549843782485\n",
      "Average loss at step  700  for last 100 steps: 0.537185618877\n",
      "Average loss at step  800  for last 100 steps: 0.521604892313\n",
      "Average loss at step  900  for last 100 steps: 0.521821700633\n",
      "Average loss at step  1000  for last 100 steps: 0.522713835537\n",
      "Average loss at step  1100  for last 100 steps: 0.519074311852\n",
      "Average loss at step  1200  for last 100 steps: 0.50379158318\n",
      "Average loss at step  1300  for last 100 steps: 0.513185833097\n",
      "Average loss at step  1400  for last 100 steps: 0.500792761445\n",
      "Average loss at step  1500  for last 100 steps: 0.511078546345\n",
      "Average loss at step  1600  for last 100 steps: 0.514083741903\n",
      "Average loss at step  1700  for last 100 steps: 0.498252100945\n",
      "Average loss at step  1800  for last 100 steps: 0.504235332012\n",
      "Average loss at step  1900  for last 100 steps: 0.496229351759\n",
      "Average loss at step  2000  for last 100 steps: 0.501257865727\n",
      "Average loss at step  2100  for last 100 steps: 0.498206732869\n",
      "Average loss at step  2200  for last 100 steps: 0.486121324599\n",
      "Average loss at step  2300  for last 100 steps: 0.486007337123\n",
      "Average loss at step  2400  for last 100 steps: 0.489312933683\n",
      "Average loss at step  2500  for last 100 steps: 0.495837730765\n",
      "Average loss at step  2600  for last 100 steps: 0.487484233975\n",
      "Average loss at step  2700  for last 100 steps: 0.487620241344\n",
      "Average loss at step  2800  for last 100 steps: 0.49373363018\n",
      "Average loss at step  2900  for last 100 steps: 0.481733733714\n",
      "Average loss at step  3000  for last 100 steps: 0.491086627543\n",
      "Average loss at step  3100  for last 100 steps: 0.48978445828\n",
      "Average loss at step  3200  for last 100 steps: 0.490824184716\n",
      "Average loss at step  3300  for last 100 steps: 0.484799810052\n",
      "Average loss at step  3400  for last 100 steps: 0.494610942006\n",
      "Average loss at step  3500  for last 100 steps: 0.491804533005\n",
      "Average loss at step  3600  for last 100 steps: 0.493548126221\n",
      "Average loss at step  3700  for last 100 steps: 0.471466080248\n",
      "Average loss at step  3800  for last 100 steps: 0.487903749645\n",
      "Average loss at step  3900  for last 100 steps: 0.486396348774\n",
      "Average loss at step  4000  for last 100 steps: 0.483806385994\n",
      "Average loss at step  4100  for last 100 steps: 0.477849054337\n",
      "Average loss at step  4200  for last 100 steps: 0.503919920623\n",
      "Average loss at step  4300  for last 100 steps: 0.494381415248\n",
      "Average loss at step  4400  for last 100 steps: 0.485178958476\n",
      "Average loss at step  4500  for last 100 steps: 0.494109834135\n",
      "Average loss at step  4600  for last 100 steps: 0.483643626571\n",
      "Average loss at step  4700  for last 100 steps: 0.502577309012\n",
      "Average loss at step  4800  for last 100 steps: 0.484009485543\n",
      "Average loss at step  4900  for last 100 steps: 0.483603140712\n",
      "Average loss at step  5000  for last 100 steps: 0.490873978734\n",
      "Average loss at step  5100  for last 100 steps: 0.48435585022\n",
      "Average loss at step  5200  for last 100 steps: 0.485637646616\n",
      "Average loss at step  5300  for last 100 steps: 0.475476013422\n",
      "Average loss at step  5400  for last 100 steps: 0.481675323248\n",
      "Average loss at step  5500  for last 100 steps: 0.482965278924\n",
      "Average loss at step  5600  for last 100 steps: 0.494159602523\n",
      "Average loss at step  5700  for last 100 steps: 0.479359490871\n",
      "Average loss at step  5800  for last 100 steps: 0.496036041081\n",
      "Average loss at step  5900  for last 100 steps: 0.477814317793\n",
      "Average loss at step  6000  for last 100 steps: 0.47615621835\n",
      "Average loss at step  6100  for last 100 steps: 0.490032158196\n",
      "Average loss at step  6200  for last 100 steps: 0.473429253995\n",
      "Average loss at step  6300  for last 100 steps: 0.48717766434\n",
      "Average loss at step  6400  for last 100 steps: 0.494755316675\n",
      "Average loss at step  6500  for last 100 steps: 0.484454902709\n",
      "Average loss at step  6600  for last 100 steps: 0.482838022709\n",
      "Average loss at step  6700  for last 100 steps: 0.472526331842\n",
      "Average loss at step  6800  for last 100 steps: 0.487658598721\n",
      "Average loss at step  6900  for last 100 steps: 0.488094297945\n",
      "Average loss at step  7000  for last 100 steps: 0.481904960573\n",
      "Average loss at step  7100  for last 100 steps: 0.478399534523\n",
      "Average loss at step  7200  for last 100 steps: 0.468901192546\n",
      "Average loss at step  7300  for last 100 steps: 0.482883302569\n",
      "Average loss at step  7400  for last 100 steps: 0.479651201665\n",
      "Average loss at step  7500  for last 100 steps: 0.472746683806\n",
      "Average loss at step  7600  for last 100 steps: 0.495399592221\n",
      "Average loss at step  7700  for last 100 steps: 0.495376369953\n",
      "Average loss at step  7800  for last 100 steps: 0.474446155876\n",
      "Average loss at step  7900  for last 100 steps: 0.491492476165\n",
      "Average loss at step  8000  for last 100 steps: 0.472425459325\n",
      "Average loss at step  8100  for last 100 steps: 0.477026080191\n",
      "Average loss at step  8200  for last 100 steps: 0.471793206036\n",
      "Average loss at step  8300  for last 100 steps: 0.491739221513\n",
      "Average loss at step  8400  for last 100 steps: 0.479001141191\n",
      "Average loss at step  8500  for last 100 steps: 0.475491408408\n",
      "Average loss at step  8600  for last 100 steps: 0.47836202383\n",
      "Average loss at step  8700  for last 100 steps: 0.484037471116\n",
      "Average loss at step  8800  for last 100 steps: 0.490142608583\n",
      "Average loss at step  8900  for last 100 steps: 0.484388482273\n",
      "Average loss at step  9000  for last 100 steps: 0.480265012383\n",
      "Average loss at step  9100  for last 100 steps: 0.496468271315\n",
      "Average loss at step  9200  for last 100 steps: 0.503229477704\n",
      "Average loss at step  9300  for last 100 steps: 0.483831109107\n",
      "Average loss at step  9400  for last 100 steps: 0.488750687838\n",
      "Average loss at step  9500  for last 100 steps: 0.471350394785\n",
      "Average loss at step  9600  for last 100 steps: 0.476616098583\n",
      "Average loss at step  9700  for last 100 steps: 0.481868239939\n",
      "Average loss at step  9800  for last 100 steps: 0.485249755383\n",
      "Average loss at step  9900  for last 100 steps: 0.482448357046\n",
      "Average loss at step  10000  for last 100 steps: 0.477261847258\n",
      "Average loss at step  10100  for last 100 steps: 0.480676804185\n",
      "Average loss at step  10200  for last 100 steps: 0.471760744452\n",
      "Average loss at step  10300  for last 100 steps: 0.488372024298\n",
      "Average loss at step  10400  for last 100 steps: 0.48764888227\n",
      "Average loss at step  10500  for last 100 steps: 0.484111512601\n",
      "Average loss at step  10600  for last 100 steps: 0.471550216377\n",
      "Average loss at step  10700  for last 100 steps: 0.490248263478\n",
      "Average loss at step  10800  for last 100 steps: 0.483774083257\n",
      "Average loss at step  10900  for last 100 steps: 0.475253650844\n",
      "Average loss at step  11000  for last 100 steps: 0.469331084788\n",
      "Average loss at step  11100  for last 100 steps: 0.489354424477\n",
      "Average loss at step  11200  for last 100 steps: 0.480743318796\n",
      "Average loss at step  11300  for last 100 steps: 0.486121980846\n",
      "Average loss at step  11400  for last 100 steps: 0.467766190618\n",
      "Average loss at step  11500  for last 100 steps: 0.470463783741\n",
      "Average loss at step  11600  for last 100 steps: 0.500812315941\n",
      "Average loss at step  11700  for last 100 steps: 0.486497858465\n",
      "Average loss at step  11800  for last 100 steps: 0.471828066409\n",
      "Average loss at step  11900  for last 100 steps: 0.49654794544\n",
      "Average loss at step  12000  for last 100 steps: 0.476750140339\n",
      "Average loss at step  12100  for last 100 steps: 0.483010114729\n",
      "Average loss at step  12200  for last 100 steps: 0.48530857414\n",
      "Average loss at step  12300  for last 100 steps: 0.467069278061\n",
      "Average loss at step  12400  for last 100 steps: 0.476670585871\n",
      "Average loss at step  12500  for last 100 steps: 0.476201213896\n",
      "Average loss at step  12600  for last 100 steps: 0.475484941602\n",
      "Average loss at step  12700  for last 100 steps: 0.494156500697\n",
      "Average loss at step  12800  for last 100 steps: 0.484999036193\n",
      "Average loss at step  12900  for last 100 steps: 0.484232792556\n",
      "Average loss at step  13000  for last 100 steps: 0.471577172279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  13100  for last 100 steps: 0.48530619055\n",
      "Average loss at step  13200  for last 100 steps: 0.482764700353\n",
      "Average loss at step  13300  for last 100 steps: 0.470374871194\n",
      "Average loss at step  13400  for last 100 steps: 0.491404855847\n",
      "Average loss at step  13500  for last 100 steps: 0.483609328568\n",
      "Average loss at step  13600  for last 100 steps: 0.479174692631\n",
      "Average loss at step  13700  for last 100 steps: 0.469367487729\n",
      "Average loss at step  13800  for last 100 steps: 0.473467061222\n",
      "Average loss at step  13900  for last 100 steps: 0.479055738449\n",
      "Average loss at step  14000  for last 100 steps: 0.469298233688\n",
      "Average loss at step  14100  for last 100 steps: 0.483972832859\n",
      "Average loss at step  14200  for last 100 steps: 0.47673127979\n",
      "Average loss at step  14300  for last 100 steps: 0.480842702389\n",
      "Average loss at step  14400  for last 100 steps: 0.483309702277\n",
      "Average loss at step  14500  for last 100 steps: 0.478387848735\n",
      "Average loss at step  14600  for last 100 steps: 0.497130485773\n",
      "Average loss at step  14700  for last 100 steps: 0.47753354758\n",
      "Average loss at step  14800  for last 100 steps: 0.473352021575\n",
      "Average loss at step  14900  for last 100 steps: 0.470769530535\n",
      "Average loss at step  15000  for last 100 steps: 0.491766757965\n",
      "Average loss at step  15100  for last 100 steps: 0.473863737583\n",
      "Average loss at step  15200  for last 100 steps: 0.485875726044\n",
      "Average loss at step  15300  for last 100 steps: 0.477093277574\n",
      "Average loss at step  15400  for last 100 steps: 0.484553274214\n",
      "Average loss at step  15500  for last 100 steps: 0.483156585693\n",
      "Average loss at step  15600  for last 100 steps: 0.471438671947\n",
      "Average loss at step  15700  for last 100 steps: 0.48119559586\n",
      "Average loss at step  15800  for last 100 steps: 0.493811255991\n",
      "Average loss at step  15900  for last 100 steps: 0.47717163831\n",
      "Average loss at step  16000  for last 100 steps: 0.480224848688\n",
      "Average loss at step  16100  for last 100 steps: 0.476362897158\n",
      "Average loss at step  16200  for last 100 steps: 0.473752984703\n",
      "Average loss at step  16300  for last 100 steps: 0.489993939698\n",
      "Average loss at step  16400  for last 100 steps: 0.475105125308\n",
      "Average loss at step  16500  for last 100 steps: 0.492349411845\n",
      "Average loss at step  16600  for last 100 steps: 0.488152126223\n",
      "Average loss at step  16700  for last 100 steps: 0.473550093174\n",
      "Average loss at step  16800  for last 100 steps: 0.488730103672\n",
      "Average loss at step  16900  for last 100 steps: 0.489326141477\n",
      "Average loss at step  17000  for last 100 steps: 0.474328394234\n",
      "Average loss at step  17100  for last 100 steps: 0.476247269809\n",
      "Average loss at step  17200  for last 100 steps: 0.486146709621\n",
      "Average loss at step  17300  for last 100 steps: 0.459989516735\n",
      "Average loss at step  17400  for last 100 steps: 0.476018445194\n",
      "Average loss at step  17500  for last 100 steps: 0.492143613994\n",
      "Average loss at step  17600  for last 100 steps: 0.471887562573\n",
      "Average loss at step  17700  for last 100 steps: 0.485599086881\n",
      "Average loss at step  17800  for last 100 steps: 0.467332257032\n",
      "Average loss at step  17900  for last 100 steps: 0.466211637557\n",
      "Average loss at step  18000  for last 100 steps: 0.4854557845\n",
      "Average loss at step  18100  for last 100 steps: 0.476769506335\n",
      "Average loss at step  18200  for last 100 steps: 0.473901513815\n",
      "Average loss at step  18300  for last 100 steps: 0.469543938041\n",
      "Average loss at step  18400  for last 100 steps: 0.471614420414\n",
      "Average loss at step  18500  for last 100 steps: 0.465021088123\n",
      "Average loss at step  18600  for last 100 steps: 0.475542480052\n",
      "Average loss at step  18700  for last 100 steps: 0.470540232658\n",
      "Average loss at step  18800  for last 100 steps: 0.485530729294\n",
      "Average loss at step  18900  for last 100 steps: 0.47267986685\n",
      "Average loss at step  19000  for last 100 steps: 0.477138623297\n",
      "Average loss at step  19100  for last 100 steps: 0.47366335094\n",
      "Average loss at step  19200  for last 100 steps: 0.485413160175\n",
      "Average loss at step  19300  for last 100 steps: 0.484356541634\n",
      "Average loss at step  19400  for last 100 steps: 0.473642424941\n",
      "Average loss at step  19500  for last 100 steps: 0.473304720521\n",
      "Average loss at step  19600  for last 100 steps: 0.474929068387\n",
      "Average loss at step  19700  for last 100 steps: 0.476906614006\n",
      "Average loss at step  19800  for last 100 steps: 0.484716235101\n",
      "Average loss at step  19900  for last 100 steps: 0.479214596152\n",
      "Average loss at step  20000  for last 100 steps: 0.480385644138\n",
      "Average loss at step  20100  for last 100 steps: 0.477428097427\n",
      "Average loss at step  20200  for last 100 steps: 0.48939969331\n",
      "Average loss at step  20300  for last 100 steps: 0.483927469552\n",
      "Average loss at step  20400  for last 100 steps: 0.485927363932\n",
      "Average loss at step  20500  for last 100 steps: 0.475896442831\n",
      "Average loss at step  20600  for last 100 steps: 0.486064740419\n",
      "Average loss at step  20700  for last 100 steps: 0.478835573196\n",
      "Average loss at step  20800  for last 100 steps: 0.473079793751\n",
      "Average loss at step  20900  for last 100 steps: 0.47531290561\n",
      "Average loss at step  21000  for last 100 steps: 0.490487633646\n",
      "Average loss at step  21100  for last 100 steps: 0.468101635873\n",
      "Average loss at step  21200  for last 100 steps: 0.479972224832\n",
      "Average loss at step  21300  for last 100 steps: 0.475033801198\n",
      "Average loss at step  21400  for last 100 steps: 0.469091620743\n",
      "Average loss at step  21500  for last 100 steps: 0.480861882567\n",
      "Average loss at step  21600  for last 100 steps: 0.474656368792\n",
      "Average loss at step  21700  for last 100 steps: 0.499407824576\n",
      "Average loss at step  21800  for last 100 steps: 0.469458906651\n",
      "Average loss at step  21900  for last 100 steps: 0.466355938911\n",
      "Average loss at step  22000  for last 100 steps: 0.482040626407\n",
      "Average loss at step  22100  for last 100 steps: 0.485447134674\n",
      "Average loss at step  22200  for last 100 steps: 0.486200201213\n",
      "Average loss at step  22300  for last 100 steps: 0.488186907768\n",
      "Average loss at step  22400  for last 100 steps: 0.489068961143\n",
      "Average loss at step  22500  for last 100 steps: 0.472313418388\n",
      "Average loss at step  22600  for last 100 steps: 0.463462507129\n",
      "Average loss at step  22700  for last 100 steps: 0.486421729326\n",
      "Average loss at step  22800  for last 100 steps: 0.472082012296\n",
      "Average loss at step  22900  for last 100 steps: 0.474980107844\n",
      "Average loss at step  23000  for last 100 steps: 0.488242565393\n",
      "Average loss at step  23100  for last 100 steps: 0.459809317589\n",
      "Average loss at step  23200  for last 100 steps: 0.463283556402\n",
      "Average loss at step  23300  for last 100 steps: 0.466052345634\n",
      "Average loss at step  23400  for last 100 steps: 0.468058685809\n",
      "Average loss at step  23500  for last 100 steps: 0.47857026875\n",
      "Average loss at step  23600  for last 100 steps: 0.481778575778\n",
      "Average loss at step  23700  for last 100 steps: 0.488010477275\n",
      "Average loss at step  23800  for last 100 steps: 0.4722469154\n",
      "Average loss at step  23900  for last 100 steps: 0.465928663611\n",
      "Average loss at step  24000  for last 100 steps: 0.457510081828\n",
      "Average loss at step  24100  for last 100 steps: 0.477494667172\n",
      "Average loss at step  24200  for last 100 steps: 0.485998841822\n",
      "Average loss at step  24300  for last 100 steps: 0.464873366505\n",
      "Average loss at step  24400  for last 100 steps: 0.482507869899\n",
      "Average loss at step  24500  for last 100 steps: 0.480779487491\n",
      "Average loss at step  24600  for last 100 steps: 0.476130240262\n",
      "Average loss at step  24700  for last 100 steps: 0.484239418507\n",
      "Average loss at step  24800  for last 100 steps: 0.478487519622\n",
      "Average loss at step  24900  for last 100 steps: 0.481420277953\n",
      "Average loss at step  25000  for last 100 steps: 0.468648139238\n",
      "Average loss at step  25100  for last 100 steps: 0.4692904827\n",
      "Average loss at step  25200  for last 100 steps: 0.46954523474\n",
      "Average loss at step  25300  for last 100 steps: 0.486104557514\n",
      "Average loss at step  25400  for last 100 steps: 0.478163561821\n",
      "Average loss at step  25500  for last 100 steps: 0.470529561341\n",
      "Average loss at step  25600  for last 100 steps: 0.47432453692\n",
      "Average loss at step  25700  for last 100 steps: 0.4713616395\n",
      "Average loss at step  25800  for last 100 steps: 0.466797215044\n",
      "Average loss at step  25900  for last 100 steps: 0.466014707685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  26000  for last 100 steps: 0.481592034101\n",
      "Average loss at step  26100  for last 100 steps: 0.470720101893\n",
      "Average loss at step  26200  for last 100 steps: 0.477469372153\n",
      "Average loss at step  26300  for last 100 steps: 0.476536405981\n",
      "Average loss at step  26400  for last 100 steps: 0.477720379233\n",
      "Average loss at step  26500  for last 100 steps: 0.483055192828\n",
      "Average loss at step  26600  for last 100 steps: 0.479705676138\n",
      "Average loss at step  26700  for last 100 steps: 0.470343339145\n",
      "Average loss at step  26800  for last 100 steps: 0.484372113943\n",
      "Average loss at step  26900  for last 100 steps: 0.477451891601\n",
      "Average loss at step  27000  for last 100 steps: 0.470957258046\n",
      "Average loss at step  27100  for last 100 steps: 0.468472120166\n",
      "Average loss at step  27200  for last 100 steps: 0.46313203007\n",
      "Average loss at step  27300  for last 100 steps: 0.458725501597\n",
      "Average loss at step  27400  for last 100 steps: 0.474561680853\n",
      "Average loss at step  27500  for last 100 steps: 0.486986271739\n",
      "Average loss at step  27600  for last 100 steps: 0.469088229537\n",
      "Average loss at step  27700  for last 100 steps: 0.482215305269\n",
      "Average loss at step  27800  for last 100 steps: 0.47661398679\n",
      "Average loss at step  27900  for last 100 steps: 0.47105083704\n",
      "Average loss at step  28000  for last 100 steps: 0.454197345972\n",
      "Average loss at step  28100  for last 100 steps: 0.476527936757\n",
      "Average loss at step  28200  for last 100 steps: 0.461276515722\n",
      "Average loss at step  28300  for last 100 steps: 0.475103920996\n",
      "Average loss at step  28400  for last 100 steps: 0.472411527485\n",
      "Average loss at step  28500  for last 100 steps: 0.472531822324\n",
      "Average loss at step  28600  for last 100 steps: 0.479642765224\n",
      "Average loss at step  28700  for last 100 steps: 0.473560276926\n",
      "Average loss at step  28800  for last 100 steps: 0.485135158002\n",
      "Average loss at step  28900  for last 100 steps: 0.478583705127\n",
      "Average loss at step  29000  for last 100 steps: 0.478708363473\n",
      "Average loss at step  29100  for last 100 steps: 0.462405096591\n",
      "Average loss at step  29200  for last 100 steps: 0.478576303422\n",
      "Average loss at step  29300  for last 100 steps: 0.469962191284\n",
      "Average loss at step  29400  for last 100 steps: 0.470189929605\n",
      "Average loss at step  29500  for last 100 steps: 0.484189637899\n",
      "Average loss at step  29600  for last 100 steps: 0.465479595363\n",
      "Average loss at step  29700  for last 100 steps: 0.467652879655\n",
      "Average loss at step  29800  for last 100 steps: 0.474472317994\n",
      "Average loss at step  29900  for last 100 steps: 0.46574010849\n",
      "Average loss at step  30000  for last 100 steps: 0.479240479171\n",
      "Average loss at step  30100  for last 100 steps: 0.481480458379\n",
      "Average loss at step  30200  for last 100 steps: 0.471606490016\n",
      "Average loss at step  30300  for last 100 steps: 0.476136977077\n",
      "Average loss at step  30400  for last 100 steps: 0.476327897757\n",
      "Average loss at step  30500  for last 100 steps: 0.46963865459\n",
      "Average loss at step  30600  for last 100 steps: 0.474318521023\n",
      "Average loss at step  30700  for last 100 steps: 0.466903144121\n",
      "Average loss at step  30800  for last 100 steps: 0.473883781731\n",
      "Average loss at step  30900  for last 100 steps: 0.460250713527\n",
      "Average loss at step  31000  for last 100 steps: 0.460016171038\n",
      "Average loss at step  31100  for last 100 steps: 0.474338356256\n",
      "Average loss at step  31200  for last 100 steps: 0.469761229753\n",
      "Average loss at step  31300  for last 100 steps: 0.47448703438\n",
      "Average loss at step  31400  for last 100 steps: 0.46938673228\n",
      "Average loss at step  31500  for last 100 steps: 0.463662887216\n",
      "Average loss at step  31600  for last 100 steps: 0.468480045795\n",
      "Average loss at step  31700  for last 100 steps: 0.479036030471\n",
      "Average loss at step  31800  for last 100 steps: 0.468128018677\n",
      "Average loss at step  31900  for last 100 steps: 0.466790850163\n",
      "Average loss at step  32000  for last 100 steps: 0.467146410346\n",
      "Average loss at step  32100  for last 100 steps: 0.47158297509\n",
      "Average loss at step  32200  for last 100 steps: 0.470662824512\n",
      "Average loss at step  32300  for last 100 steps: 0.466956532598\n",
      "Average loss at step  32400  for last 100 steps: 0.463489636183\n",
      "Average loss at step  32500  for last 100 steps: 0.460486963689\n",
      "Average loss at step  32600  for last 100 steps: 0.484496479928\n",
      "Average loss at step  32700  for last 100 steps: 0.487862746119\n",
      "Average loss at step  32800  for last 100 steps: 0.470388192832\n",
      "Average loss at step  32900  for last 100 steps: 0.471123389304\n",
      "Average loss at step  33000  for last 100 steps: 0.480155570805\n",
      "Average loss at step  33100  for last 100 steps: 0.469690870047\n",
      "Average loss at step  33200  for last 100 steps: 0.475128427446\n",
      "Average loss at step  33300  for last 100 steps: 0.466936113238\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xl8XHW5+PHPM5NM9n3plqTpSlvo\nSgrIjthaQAEVEdzQq+JVUe91hasCAhdRr+uVe4WLFfSngIBAkaW07AKlG93XdE/apmn2PZnJ9/fH\nWXJmMpmkNG3SyfN+vfLKzJkzM985yTzne57vc75HjDEopZQaGXxD3QCllFInjwZ9pZQaQTToK6XU\nCKJBXymlRhAN+kopNYJo0FdKqRFEg75SSo0gGvSVUmoE0aCvlFIjSMJQNyBSfn6+KS0tHepmKKXU\nKWXNmjVHjTEF/a03oKAvIouA3wB+4AFjzD0Rj/8KuMS+mwoUGmOy7cdCwEb7sf3GmCtjvVdpaSmr\nV68eSLOUUkrZRGTfQNbrN+iLiB+4F1gAVACrRGSJMWaLs44x5t89638dmOt5iTZjzJyBNlwppdSJ\nM5Cc/llAuTFmtzGmE3gEuCrG+tcDDw9G45RSSg2ugQT9ccABz/0Ke1kvIjIemAC87FmcLCKrRWSF\niFz9nluqlFLquA32QO51wOPGmJBn2XhjTKWITAReFpGNxphd3ieJyI3AjQAlJSWD3CSllFKOgfT0\nK4Fiz/0ie1k01xGR2jHGVNq/dwOvEp7vd9a53xhTZowpKyjod/BZKaXUezSQoL8KmCIiE0QkgBXY\nl0SuJCLTgBzgbc+yHBFJsm/nA+cBWyKfq5RS6uToN71jjAmKyE3AUqySzcXGmM0icgew2hjj7ACu\nAx4x4Zfimg7cJyLdWDuYe7xVP0oppU4uGW6XSywrKzPvpU6/pSPIfa/t4pJphcwtyTkBLVNKqeFL\nRNYYY8r6Wy9upmFo7wrx25fL2VDRMNRNUUqpYStugr7fJwCEuofXkYtSSg0ncRP0fXbQ7x5m6Sql\nlBpO4ifoiwZ9pZTqT9wEfb846Z0hbohSSg1jcRP0ffYn0Z6+Ukr1LW6Cfk9PX4O+Ukr1JX6Cvg7k\nKqVUv+Im6IszkKs9faWU6lPcBH2wevsh7ekrpVSf4ivoi2j1jlJKxRBXQd/n05y+UkrFEldB3+rp\na9BXSqm+xFXQ9/lEe/pKKRVDfAV9Ea3eUUqpGOIq6Gv1jlJKxRZXQd+n1TtKKRVTXAV9v09PzlJK\nqVjiK+iLDuQqpVQscRX0RTSnr5RSscRV0Pf7tHpHKaViibugH9KYr5RSfYqroO8THchVSqlY4iro\n+306DYNSSsUSV0Hfp9U7SikVkwZ9pZQaQQYU9EVkkYhsF5FyEbk5yuO/EpF19s8OEan3PHaDiOy0\nf24YzMZH0vSOUkrFltDfCiLiB+4FFgAVwCoRWWKM2eKsY4z5d8/6Xwfm2rdzgduAMsAAa+zn1g3q\np7D5tHpHKaViGkhP/yyg3Biz2xjTCTwCXBVj/euBh+3bHwSWGWNq7UC/DFh0PA2Oxa/VO0opFdNA\ngv444IDnfoW9rBcRGQ9MAF4+lueKyI0islpEVldXVw+k3VH5dT59pZSKabAHcq8DHjfGhI7lScaY\n+40xZcaYsoKCgvf85qJXzlJKqZgGEvQrgWLP/SJ7WTTX0ZPaOdbnHjedcE0ppWIbSNBfBUwRkQki\nEsAK7EsiVxKRaUAO8LZn8VJgoYjkiEgOsNBedkJo9Y5SSsXWb/WOMSYoIjdhBWs/sNgYs1lE7gBW\nG2OcHcB1wCPG9HS1jTG1InIn1o4D4A5jTO3gfoQeWr2jlFKx9Rv0AYwxzwHPRSy7NeL+7X08dzGw\n+D2275ho9Y5SSsUWV2fkavWOUkrFFldBX6t3lFIqtrgK+lq9o5RSscVX0NfqHaWUiimugr7PJ2jM\nV0qpvsVV0PcLmt5RSqkY4iro+3QgVymlYoqvoO8TrdNXSqkY4iro+0UIaXpHKaX6FFdB3+cTQt1D\n3QqllBq+4iro+306kKuUUrHEV9DXk7OUUiqmuAr6Og2DUkrFFldB36/VO0opFVPcBX2t3lFKqb7F\nVdD3idCt1TtKKdWnuAr6Wr2jlFKxxVXQ9+nJWUopFVPcBX1jwGjgV0qpqOIq6Pt9AqBlm0op1Yf4\nDPra01dKqajiKuj7xAr6WsGjlFLRxVXQ99ufRit4lFIqurgK+k5PX9M7SikVXVwGfZ2KQSmlohtQ\n0BeRRSKyXUTKReTmPta5VkS2iMhmEfmrZ3lIRNbZP0sGq+HRaPWOUkrFltDfCiLiB+4FFgAVwCoR\nWWKM2eJZZwpwC3CeMaZORAo9L9FmjJkzyO2OyqfVO0opFdNAevpnAeXGmN3GmE7gEeCqiHW+BNxr\njKkDMMYcGdxmDozfTu9ozFdKqegGEvTHAQc89yvsZV5Tgaki8qaIrBCRRZ7HkkVktb386uNsb0x2\nR1/TO0op1Yd+0zvH8DpTgIuBIuB1EZlpjKkHxhtjKkVkIvCyiGw0xuzyPllEbgRuBCgpKXnPjfBp\nTl8ppWIaSE+/Eij23C+yl3lVAEuMMV3GmD3ADqydAMaYSvv3buBVYG7kGxhj7jfGlBljygoKCo75\nQzic9I7W6SulVHQDCfqrgCkiMkFEAsB1QGQVzlNYvXxEJB8r3bNbRHJEJMmz/DxgCyeIVu8opVRs\n/aZ3jDFBEbkJWAr4gcXGmM0icgew2hizxH5soYhsAULAd40xNSJyLnCfiHRj7WDu8Vb9DDYnvaM9\nfaWUim5AOX1jzHPAcxHLbvXcNsC37B/vOm8BM4+/mQPjDORqR18ppaKLqzNynZy+pneUUiq6uAr6\nWr2jlFKxxVXQ1+odpZSKLb6Cvvb0lVIqprgK+j3VO0PcEKWUGqbiK+i71Tsa9ZVSKpq4CvpavaOU\nUrHFVdB30zsa9JVSKqq4Cvp+nU9fKaViiqug79P0jlJKxRRnQd/6rR19pZSKLq6CvtbpK6VUbHEV\n9N30jnb1lVIqqrgK+n6t3lFKqZjiMugHNegrpVRUcRX0kxKsj9MV6h7iliil1PAUZ0HfD0BHUIO+\nUkpFE2dB3/o4HV2hIW6JUkoNT/EV9BPtoK89faWUiiqugn7Ar0FfKaViiaugn+D3keATOoKa3lFK\nqWjiKuiDldfv6NKevlJKRRN/QT/Rr+kdpZTqQ/wF/QSfpneUUqoPcRr0taevlFLRDCjoi8giEdku\nIuUicnMf61wrIltEZLOI/NWz/AYR2Wn/3DBYDe9LUoJfc/pKKdWHhP5WEBE/cC+wAKgAVonIEmPM\nFs86U4BbgPOMMXUiUmgvzwVuA8oAA6yxn1s3+B/FkpSo6R2llOrLQHr6ZwHlxpjdxphO4BHgqoh1\nvgTc6wRzY8wRe/kHgWXGmFr7sWXAosFpenRJCT7ataevlFJRDSTojwMOeO5X2Mu8pgJTReRNEVkh\nIouO4bmDKinBrz19pZTqQ7/pnWN4nSnAxUAR8LqIzBzok0XkRuBGgJKSkuNqSHKij7pW7ekrpVQ0\nA+npVwLFnvtF9jKvCmCJMabLGLMH2IG1ExjIczHG3G+MKTPGlBUUFBxL+3uxevoa9JVSKpqBBP1V\nwBQRmSAiAeA6YEnEOk9h9fIRkXysdM9uYCmwUERyRCQHWGgvO2G0Tl8ppfrWb3rHGBMUkZuwgrUf\nWGyM2SwidwCrjTFL6AnuW4AQ8F1jTA2AiNyJteMAuMMYU3siPogjKVGnYVBKqb4MKKdvjHkOeC5i\n2a2e2wb4lv0T+dzFwOLja+bAaXpHKaX6Fqdn5Gp6RymloonToN+NdfChlFLKK/6CfqIfY6ArpEFf\nKaUixV/Qd66TqykepZTqJY6Dvg7mKqVUpDgM+n5Ag75SSkUTf0E/0e7pd2l6RymlIsVf0Nf0jlJK\n9SkOg76md5RSqi/xF/Tt9E5rZ3CIW6KUUsNP3AX9ouxUAPbVtA5xS5RSaviJv6Cfk0JKop+dVc1D\n3RSllBp24i7o+3zC5MJ0dh5pGuqmKKXUsBN3QR9gyqh0dlRp0FdKqUhxGfSnjsqgqrGDhrauoW6K\nUkoNK3EZ9EvzrMHcA7U6mKuUUl5xGfRz05IAqGvtHOKWKKXU8BKXQT8nNRGA2hYN+kop5RWXQT87\nNQBAfavm9JVSyitOg77V09f0jlJKhYvLoJ/o95GRnKA9faWUihCXQR8gJzWgOX2llIoQx0E/UdM7\nSikVIX6DflpA0ztKKRUhfoO+pneUUqqXuA362amJ1Gt6Rymlwgwo6IvIIhHZLiLlInJzlMc/JyLV\nIrLO/vmi57GQZ/mSwWx8LLmpAVo6Q3TqFbSUUsqV0N8KIuIH7gUWABXAKhFZYozZErHqo8aYm6K8\nRJsxZs7xN/XYjMtJAeDd/XWcPTHvZL+9UkoNSwPp6Z8FlBtjdhtjOoFHgKtObLOO32VnjCE3LcD/\nvrZrqJuilFLDxkCC/jjggOd+hb0s0sdEZIOIPC4ixZ7lySKyWkRWiMjV0d5ARG6011ldXV098NbH\nkBLw84n5xby6vZr2rtCgvKZSSp3qBmsg9xmg1BgzC1gGPOR5bLwxpgz4JPBrEZkU+WRjzP3GmDJj\nTFlBQcEgNQkm5KcBcKSxY9BeUymlTmUDCfqVgLfnXmQvcxljaowxTmR9ADjT81il/Xs38Cow9zja\ne0xGZyYDcLix/WS9pVJKDWsDCfqrgCkiMkFEAsB1QFgVjoiM8dy9EthqL88RkST7dj5wHhA5AHzC\njM7SoK+UUl79Vu8YY4IichOwFPADi40xm0XkDmC1MWYJ8A0RuRIIArXA5+ynTwfuE5FurB3MPVGq\nfk6YUXZPv6pBg75SSsEAgj6AMeY54LmIZbd6bt8C3BLleW8BM4+zje9ZZnICKYl+7ekrpZQtbs/I\nBRARRmcla9BXSilbXAd9gFGZSZreUUopW9wH/dGZ2tNXSilH3Af9UZnJVDd1YIwZ6qYopdSQi/ug\nn5ceoCPYTUunnpWrlFLxH/TTkgCoadazcpVSKv6DfnoAgKPNOre+UkrFfdDPT9eevlJKOeI+6Ds9\n/Rq9dKJSSsV/0M9Ns4O+9vSVUir+g35Sgp+M5ATt6SulFCMg6IOV16/RgVyllBoZQT83LcA/y49y\n01/X8tBbe4e6OUopNWRGRNAfnZlMbUsn/9hwiLue3UJnsHuom6SUUkNiRAT97y+axm+um8PdH5lJ\nV8jw2o7BuQ6vUkqdakZE0C/JS+WqOeP4eFkRGckJvLr9yFA3SSmlhsSICPqORL+PsVkpVDdp+aZS\namQaUUEfID8joOWbSqkRa8QF/by0JI42d7D9cBOdwW5W7qll0a9fp75VdwRKqfg38oJ+eoB9Na1c\n/ts3eGJtBZ/8vxVsO9zE9sNNQ900pZQ64UZc0HcmYAt1G17fUU2w27q4Sn1b11A2SymlTooRGPQD\n7u1XPFU8esauUmokGHFB37moCkB7V89JWrUtWtGjlIp/CUPdgJMtPyMp7H5uWoCuYLdeZEUpNSKM\nwJ5+IOx+cU4KeekBqps7aGzXvL5SKr4NKOiLyCIR2S4i5SJyc5THPyci1SKyzv75ouexG0Rkp/1z\nw2A2/r1wBnIdRbmp5KYFeHbDIS762SuDPi/P0s2Hufa+t+m2B4xPFeVHmlm9t3aom6GUGmT9Bn0R\n8QP3ApcBM4DrRWRGlFUfNcbMsX8esJ+bC9wGnA2cBdwmIjmD1vr3ICXg59YPzeBbC6YCUJKbSp69\nI6hr7WJXdTPtXaFBe7+1++tYuaeWqqZ2d9n196/ggTd2D9p7nAgf+OVrXPP7t4e6GUqpQTaQnv5Z\nQLkxZrcxphN4BLhqgK//QWCZMabWGFMHLAMWvbemDp5/OX8C50/JB6A4J5Xc1J6Uz+f/uIqP/M9b\ng9Yzb+kIArDnaAsAxhjW7K9jY2XDoLz+iWDMiTsqefLdCn65bMcJe32lVGwDCfrjgAOe+xX2skgf\nE5ENIvK4iBQfy3NF5EYRWS0iq6urT84MmGeMzeLz55WyYMYoaj1n4x5ubGfrocZBm4mzpcM6athX\n0wpAW1eIzmA3da3Dd/zgUEPPUclgp6We33iYh1fuH9TXVH0Lhrr58p9X8+7+uqFuihomBmsg9xmg\n1BgzC6s3/9CxPNkYc78xpswYU1ZQUDBITYotkODjtg+fTkFGkpvnT/CJ+/hf3hmcwNRs9/T32j39\nejvYNwzjaR+8RyFN7cFBfe22rhC1LZ2ETtIYx46qplNuPGUw1bR0snRzFSt26/iMsgwk6FcCxZ77\nRfYylzGmxhjjFLo/AJw50OcOBz+4YjoPfn4+l0wrBODM8TnsPtrsPv5m+VFaO99b8HPSO3trwoP+\nQHr6/9x5lE3HmQaqa+l02zBQmz3v2TDIZyq3doYIdRvqWju5+YkN/H1txaC+vldFXSsf/PXrLN9a\ndcLeY7hzOh3v9f9XxZ+BBP1VwBQRmSAiAeA6YIl3BREZ47l7JbDVvr0UWCgiOfYA7kJ72bCSnpTA\nxacV8tG54/jMOeOZXZTN4YZ2jDFU1LXyqQfe4a8xev7Lt1T1WeniBv2jrTS1d3GgzkrzxJrgram9\ni6PNHdz69CZ+vXxn2GM1zR3c/dzWAVUZvbO7hrl3LmPWj190v/wDUVHf5t6ubxvcI5LWTivdVd3U\nwbMbDvF6H2m0ts4QVY3tUR8bKOtvGJ6uGmma252gP3jFCerU1m/QN8YEgZuwgvVW4G/GmM0icoeI\nXGmv9g0R2Swi64FvAJ+zn1sL3Im141gF3GEvG5YumzmGO68+g7HZybR2hmhsD7LuQD1gpQlW761l\n9o9f5Ghz+Nm7X/zTaq75/dvc+KfVvYKYE2z31bbwzUfW8eU/rwGgsT1IMBQ9cN++ZAv/8uAqqps7\nqIvYOTy38RD3v76b9RX1/X4eZxwh1G2O6RoCtZ6pp+sHeezB6XEeaeqguTPY55HEFx5axdl3v3Rc\ng8puKm0ARyt1LZ2cd8/L7t87XvT09DXoK8uAcvrGmOeMMVONMZOMMf9pL7vVGLPEvn2LMeZ0Y8xs\nY8wlxphtnucuNsZMtn/+eGI+xuAanZUMwKGGNjZWWKmO8iPN/PSFbTS0dfHu/nqa7BO5OoI9X6YX\nt1T1Slc0dwTx+4T2rm7+WX407LHGPvLl5Uea2HWkmab2YK+gv9WeDfRAbWu/n8N7stnOqqawwbwl\n6w+y+WD01FFtSyfjslOAE5PeAdhf04Ixfb/+W7tq3LZEWrmnts8dppcziZ4T/PcebeHPb++Nuu6O\nqiYq69vYGGVn+p3H1nPtfbHLV5/beIjvPb6+3zadbE3tmt5R4UbcGbkDMcYN+u1uj7r8SLObJvj1\n8h3MvP1FPrt4JdfetwIAvz0InBLwh71WS0eIKYXpAL1SMuVHmjnvnpd7VVZU1rfRYgdHJ2C9sbOa\n5o6gOwX0gdo2+uMdhL3n+W184aHV7v0fPrmRP7yxx73f1hnib6sO0N1tqGnuZGJBmvX+xxj0n91w\niK2HGvt8vM3+XHuOtg7o9Z1SV8fmgw1ce9/b/Hzp9n7b4qTQnB3LE2sr+NHTm6MGwMN2Kqm2pac9\nR5s72FjRwONrKli5p5aGGEc9L287wpPvVp7QctdYNlU2cPdzW3u9f4v29FUEDfpRjMmyerkHalvZ\nVNlIcqKPxvYgFXVWoN180Apqr++oZr2dDvjbl89hYkFaWO/dGENLZ5AZYzOjvs9LW6uorG/jlW09\ns322d4XC5gGqb+3kSFM7n/nDSh5ffYAdTtCv6+npB0PdUStUvEF/b00LtS2dBEPdhLoNje1BN9AB\nPPluJd97YgMr9tRQ29LJhHwr6D+z/iAbYqSSWjqC7KvpCcxf++taLvvNGwDsr2ll6g+fZ9vhxrDt\n4bQHoLGPoJ+ZbE0LFRn0nRLYgZTU9qR3woN/tEH0Kjfo96TB7n2lnM8sfse9/9K2Kv7yzj4u/cWr\nvBVx1FbT3EFXyNA2iCf2RfrCg6v47mPRjyaetdN+LRHBva+B3I5giP9+aSf3v76r12vVNHcM2c5L\nnXga9KMoyEjCJ/DQW3tp7gjyqbPH91pnTnE2P77ydPd+SW4amcmJ7K9p5aKfv8ITaypo7QxhDEwp\nzCDg772p395tpTDWV/SkWQ7Wh/fguw2UV1mVROsO1NNkf4md9M6jq/Yz47alfOUva3q9flN7FyI9\nrwNWSslJTXmD/up91lDLGzuP0tYVYnRWMoEEHyv31HLl797sc1v99uWdXPTzV/nxM5t7lWEu31pF\nZ7Cbh97aB0BHsBsnljglrA1tXVEDTEZyorVeTXjQb+6w2j6Qwdn6iGDv7GDqoqSMDjdYwb7Ws0Oo\namynvrXLLeVdvrWKHzy5iV3VLayMGLh30lCDnQ7zemnbER5bUxF1ex21x2yaIuaP6iun/93HNvCL\nZTu4+7ltYcvLjzRx5l3L+WuUcymuv38Fn/y/Fcf1GQbLnqMtUf+Op4LyI81c8l+vhnWWTiYN+lEk\n+n0UZCSxq7qFcdkpfOPSKbxvYh6XnFZAoT1L5/QxmVwxq6doKT89QEZyApsPNrCvppVvP7ae1fus\ntE1GcgIleam93meDHezf2VPDi5sPY4zhYH3vYLajyurdO683LjvFPep4buNhOoPdvLKtulfQbWoP\nUpSTErasoa3LDUxVnsC51n7tFzcfBqyJ6aJVCEVOSucE7wff2ht29bHqpg4yU6zAvXJPDf/x5Maw\nI4/99k6rK2Ro7bRq973BzAleezw7h65QN41tQfd+f5yevvPbOQp7fWc1y7eEl3FG6+k711hwLrTj\nPX/haHMHVY3tfOvRdbR1htzrLm+qbHS/zAdqW910VqSjzR3sr+k5WqtqbI85/Yf3b/HjZ7awqbKB\n25ds5lCD9X/gvL+zfRzONnfa0RXq5lfLdrBk/UEAEv2CMYaP/e9b/Gb5Tvco1hlTcRhjeHt3Tdjy\nYKibny/d1m+RQHe34Y5ntrj/x4Ph0w+8wy+W9Z/iG47u/McW9hxt6TXGd7Jo0O/Dp88ez+ljM/nW\ngqlkpSTy8I3n8MfPn8XMcVkATMhPDZu8TUTITEnEG3d/vGQzYAX9yQXpFOeGB2CwTghr7+rmxj+v\nYfPBxl49fYDtdk/fCfTzS3M42NBGe1eIGjtIdYa6KT/SHPa8po4uCjOSSfT3nHRW39rpBsyWzpBb\nHrq3ppWkBB+7qq2Aleu57kBGkpVq2VHVxJl3LuPpdT2nWjS2BSnISMIYWPxmzxjBhop6mu3Avau6\nhb++sz9spxD0bKgl6w8y785lvLTVSnM56SeAd3bX8u7+OubftZwrf/dm2E4nMsde19IZtfIosqf/\nX0u3c/PfN/Loqv3uZ4mW048cRPaOoxxt6uS2pzfz93creX1ntbuD+OYj7/K9xzdgjOGCn73Cl/60\nmmh+8tw2vvDQKvfznv/Tlzn9tqUc9uyI1+yr45a/b6C724SVrz741l5uWLySB9/ayz/WH7La09xX\nT9/5W1vbc/E/9/Cbl3ZyxcwxfH/RNLpChuaOIGv21fGr5TvcIwPnbx7tszuWbz3Cva/s4udLe44W\nDjW08XbEDqO6uYPFb+7h72uP7RSdts4Qf3p7b6/OTGewm8r6trBtFYsxhtKbn+We57f1eqy6qeOk\nnq3cEQy5qcmhOmdQg34fvn7pFJ79xgV87MyisOVOZU9pnpXz/p9PzeN/PjUP6MlDA1x/Vgm77V5q\nWiCBH35oOvd/psx9PDnR2vTXeF5/X00r+2pb3JSMY2dED+mKWWMxxuoxHG3qpGy8NYfdugPWP+/j\nayqoae6gqT1IRnICWXaPG6yB0/qwFEaH28u/bn7PeXS5nimofXZ646/v7KcrZPjNSzvdL2JtSydz\nirOZU5zt9h7B6hU3RPQ6ncDqbQ/ALX/fCOCeEOcE6Y/MHYffJ9z+zBY6Q91sPdQYFlC2RAwYn/2T\nl5h35zKCoW7+7ZF33Z6UG/TtgNhtrCD5/Sc28s1H1vHCpkNuAPH29L1B36lmAqt3fLS5gyP2JHoC\nbi6/tTPEkaYONw3XV2+uoq6VfbWtGGOV03aFDKFuwxOe6q+n3q3k4ZUHKK9u7pXOcnr2zqC5s9OJ\nPIPaGQNp6wxhjOHhlfs5qzSXez81zz1q9Y4hOXX96RFBf51nXKfLrpxyOih+n49d1dbf7rcvlXPD\n4pVhRy1O2wbS0zfG8NuXdrLnaAsvbavi1qc399qJONt9oOXEznq/f633+MXvX9vFZxevHNDrDAZv\n1V39EKWnNOgfo7H2l7/UHui8fOYYLp9ppXkyk3uC2WfO6RkHSEtKoCgnleljMnnqa+fx0rcvYum/\nXchdV5/B9xdNY+PtCwF4al0lD7yxh3klOWE9Le+XJT0pgQUzRvHpc0r4yzv7qW7uoKw0l6yURNbs\nq6OqsZ3vPLaea37/th30E8Pa1dDaFZYaqWpsZ83+OhL9wtcvneIuz0sLsOKWS/lEWTENbV20dgZ5\n8t1KRmUmsbu6hbd2WcGstrWTvLQAEwvSwlIQ5Ueae6VgnCAxPkqqC3quZOZU3Vw4NZ+LphaEVQM9\nv8lKPyX4hNd3hg/mOu//6OoDPLWuZwfUEeymvSvUZ0romQ2H3EBS12KNMXTbZw07poxKd29PG51J\njeeowjkCc9S1dsas9AFrp9MZ7KahrYuDDT3P977Wdvvv/s6eWjeN8+Dn53OBPVkgWDs+YwzVdk/f\neyTUEez5zC0dIdbur2NvTSufsHfuzo7dG4icIwafL7zn4e0NO9vFOfrYUFHPpb94jf95tZwdVU10\nhrp58K297gmLzvreIz3Hqr21YUeO2w438ctlO/jlsh0cabTaEjk5obODPtzYztl3L+cZT2cjmkrP\n0XPkeMjB+jaa2oNhOyljDH9+e6+7zQeTd6dcO0RTsWjQP0YLZozi42cWMdEO+l4Zdk8/JdHP9DEZ\n7nJvr2lOcTaTCtIZn5fGp88ZT05agIzkRHJSE1m2pYqkBB//99ky8tIDZKdawdpbEVRg987OnpAH\nWKmB/PQAF04t4MUtVe4XeM/RFirr28hITnBz6xCe3gHrC7R2Xx1njMsKS1flpQcYnZXsVh69ur2a\nhrYu/uPy6ST4hNuWbObMO5c/0y0kAAAb4klEQVRR3dRBblqA0ZnJ7nMn5qexq7qlz6BfnBs96DvB\nwSnjzE6x2hA5tjAqM4n5pbm81Mf0Cj94cpN72xmEbWjr6pXvdmysaKArZCjKSaEz1M23/raegw1t\nYYffU0f1/D1PG53B0aYOtwe7P+KciYa2rrCjhGjnFDi968ON7Ryyx3ECCT4q7KosY4y7s//RU5v4\n90fXAVBWmsuMMT3VYOVHmqlt6XS30aq9te4Jgqf98AVetivD2rpC7DpiHXmeNSEX6An63vZX2YHW\nO3VHZ7Cbf2w45N53PreTCtxppxV/9sJ21thHjfc8v82dmts5Kqmsb+uVfnrgjd3c+Y8t7v0VdnHD\n0k2H3Z1E5FQkzhFjRV0bVY0dLLXHoaI53NAedj7KwYgjJmc8wltF9sTaSn709Gb+++XyPl/3vfKe\nGV/X0snu6mb+99VdJ7VaSoP+MZo6KoOff3w2CVGqcZzgmpsWQDw5mrQkf691I5XYgXBuSQ65aQHG\nZKUwtTCjV6rHCfrewFmQkcTHzyyivrUrrOqiM9jdO+i3hff099W0sL6igTNLrBTRY//6Pj55dom7\no8qxA8OzGw/hE7j4tELmlmSzu7rF/TLnpgXctBfAvPE57K5upr61k0kFadx59RlAT4+rNEpPf1x2\nimcyOjvopyYyyrMzccYmMpMTuXR6ITuqmqmsb+PxNRV87S9rw17vA9NHAT1jB9VNHVHLKS8+rcAN\nenOKswGrfPXPK/aFreecaxFI8FGal0pTR7BXJZXDmPCS2gMRRwJODx+soOT0KOeX5lBpr1vd1BGW\nvnB2QOlJCWF/+2C3cavAAP7fiv18NiK94nCm13CCfbSg71RLeat9lm4+THVTB/960SSgJ+jvtlM6\n/U0JUus5g31nxLhTTXMnR5s73fau2F1DSqKfzlA3T9lHAH319B0r99T2GTS/+KdVfP+Jje796+5/\nmz1HW3jgjd2Eug1H7KDvlPEaY/gv+xyQvgbhY9lU2cCnHljR53Od9FlSgo+61i4eXXWAn76w7aRe\nrlWD/iByevp56daX6cKp1oyh6cn9X4rY+QI6gefuj87kp9fMIjsi/+3kYYs9VTl5aUmcNzmfvLRA\nr0PdzOTEsLGG13ZU89qOagIJPqaNzuCPb+6lM9jN2ROtI4f5pbnc/ZGZ7k4rxz7aeGXbEU4fm0VW\nSiLvm5Qf9h7enr4IzC3JpiNo5eDz05O43k4nuOmdXOsoKSnBF/YatS2dbD/c5KYqslPDjyBmFVnb\nJjMl0d1OO6qa+M5j63l2Y09PFODWD1nX+fnIXGsm78jeOFg7yzPGZrn3p3t60A++uTds3Sl2T78g\nPanX1df2RXntvZ7zC3ZFBjrPuMFf3tnPU+sqSUn0c/rYLCrq2+juNm5q52fXzOJnH5sV9nyngzDb\n3gbOALjXsi29j4IO1LaSnOgj1T6B0Nmhe9vqVO94e/r/3HmUnNREPjpvnNv+zmB32Of2/i2dIwmw\ndgjeMtgdESkep+Pwgyc3cfuSzbxVXsOiM0aT4BM67J3J/trWsHRZZNA/0tTBfnt8xNuTbu8KsfVQ\nz/uNzkzmQG0bn7jvbe56ditPvlvp9vSdlGJzR9BzJNFz1DVQr+2o5s3yGneMI5KT3inJTaWutdMd\n96uo6/8M+8GiQX8QZSRZATLHvijL7z89jz9+bj6FGcmxngb09DRmFTnVQWlMyE/jC+dPAHp6uc5r\n5aYF3C9vfkYAv0+YXJhOV8j6B3WCpXcgN8EnvLu/npV7aslKSeTasmKaOoIU56ZwyWnRp7R2Pktr\nZ4j3TbJ2DFfNGcvZni+2t6efmZzopkIONrSTlZJIgt9HRnKCOxjp9FSdXvyHZ48lOzWR13ZU88Ff\nv873Ht8AQHZKYtgRhFM5lekpgY0MqGB9oUryUtnzk8u55fJpiPQOggtnjOJDs8YwJtt6fZ9Yn2vm\nuCwWzhjlBpzM5AQykxPcs7Tz0wO9gr7T03d2kNBzxjEQ1hM/0tTOIyt7LjGxbEsVmyobGZOVTHFO\nCp3BbqqbO3h45X4Cfh8Lpo/i2vnF/Oxjs/jpx2a6nw/ggsn55Kcn8cKm3umNv60+0GvZgdpW8tKS\n3B16WsBPIMEXtkN0Bui9Pf2K+lZK89MosD93TXNnr+mxc9MC5Nudnd9dP5effNRqa3VzB7UtHWSl\nJJKS6Hd3Zg5nDOGJtRU8+NZeUgJ+vnbJJLfU2Bnb2uRJ0RyKMhHfyj21vLT1CPPvWk51UwcNbV1s\nP9wU1sZXv3sxgNu733640T36c1KKzmMBv49Ve+s4756XmXX7iwMOys56fa3vHB06Qd/Z4UYbAztR\nNOgPIieN4lx8PTWQ4E7X3J8bzrUGfueWhF9N8qsXT+aHV0znh1dYPdfCTOuLJyIU56Ta72ctcyqK\nclITmWynI9KTetI73Z4eS3tXiKvnjiMnNZFvXjo1aroKenqDAO+zjwYmFaTz6Jff5y739vSzUxOZ\nXNAz6OnscHJSA24gcQZyM5ITWH/rQn517WyyPVcvm5CfRmleKpkpPemd7NREN9h1G6vHnZLoD8s1\nAzz0L2fxwr9d4G6jwoxkysbn8OS7VqrAOer5zXVzue3DpzPWPvt6TFYKRTmpPPP187nNc9Ld7OJs\nxmanuEdieelJ5Gf0BH2fWAPFgQSfO8gPuLX6i04fzSMr97s91W//bT2/eSl85lSw8tNF9t/zV8t2\n8NzGw/zbginu9r92fjGfmF8CWDvNT5QVc9nM0ZSNz6GtK4SINZbkeGNn76qhA3WtbmB2tk9uaqDX\nCXDQU+IJVrlmUU4qWSmJ+H1CTUtHrwCVnRrgb19+Hz+8YjoFGUnu/8PhhnZqWzrJTw8wdVQ6b++q\n4SfPbaW1M0hHMBQ2sPnpc0r4xzfOZ3JhhlsoccFU66jSm+I53NDupj2TE31kpyayck8te2taaOsK\ncdNf1zL7xy+64wOO5EQ/aZ5pUrZ5jjoaWrt48M09PGBPTVJWan0PK+vbaOoI8vDK/dz7SjlHmzvc\nc0ceXrmft8qPcsvfN7hzdDkD8ZGD+w4nvVOcm8rRpk53UsTvPr6BT/Qzv9Ng6T/voAbMSe94yx0H\n6iNzi/jI3KJey30+4YsXTHQH9cZ4er7FuSnsONLkvt/4fCtoFGQkWT3hcusIYuGMUbR1hnjwrb3u\nc5vag+SmBVj7owVh4w+RnEtJ+n3CfE/vPmydtAB56Ukk+ITs1AA5aQHG56Wyr6bVE/QT2V/bs356\nUgLpSQlkpfY8DtaF61/5zsXua+elBUj0C4UZSW6vv7UziIhQkpvaa1bMiflppAbC/60/ePpoVu21\nBhjnlOSwv6bFnSPJ6el7K4rGZadwzsRcVuyu5Rcfn01nqJtEv49RmUmMyUpmSmE6507K41sLpvL1\nh9/lUEM7eWmBsFLUvTVWKuWm90/mhc2HeXp9JZ99X2mf00WfOT7HbcMjqw5wwZR8brxgYtR1/T7h\np9dYKZ/Tx2bywubDXDl7LK9ur6atK0RWSqIblL/7wdNISvBx17NbqWrs4HRPOgusnfrhiDZNG51B\nq13qGeo2HKxv40OzxuDzCblpAWqae4oBxmYlc7Chndy0RCYWpDPR3uE7O+uqRivo56YFKM1L47E1\nFWw73ERBRlLYyY0AnygrcY9krQ5MNVMKMxiX3RA2mHu0uYOinBQO1LYxOjOZyYUZrNpb63433tlj\n/aP95PltpCcl0NzRc5JibnqAFvucg1Wes6rrWju57/Xd7gB8WWmueyJaSqKfe1+xyj2dOZ/23nOF\nW2oM1g50ZtFMN9hXRjnfBqzzJlIDfvLSAr3GmLZXNdHWGeo1f9dg06A/iNyB3PRjD/r9mVKYzn9f\nP5cFM0a5y+aW5FBR1+ZO9ub09AszkvnC+RNYuukwC2eMojg3lbklOew+2tJr6udYAR+sCeSSE31M\nG53Zq3bbycPnplnppcKMJHcMYua4rLCgn+XZeSQlWGc8Z3jGGpyefuQJbD6f1VsvzEh2A4lzxFCS\nl8r2qiZmFWWxu7qF5o5gr9QLwNVzx3HXs9YlHm5eNC0swDvzLEWWkf6/L5zNoYZ2Cj1jCg9+/izy\n0gOkJSXw1y+dA1jluM62yEkNkJGUQFNHkKPNHYzOTOaMcVlMzE/jf1/dxZvlR8Mqgj46bxyNbV38\n6EMzyE4JkJWayB8/N58NFQ187tzSPo++vK6dX0xlfRu3XDadeXctA+BDs8a4V3772iWTw4JbZIck\nP+J/NTs1kRljMt1pJg43thPsNu5RSEF6Eocb292gX5SbysGG9rAjNeg5n8Xp6U/ITwurgPrjm3sp\nKw3vREwd3XOE6Az2F2QkMXNcVljQr2nuZH5pDgdq2yjMTObsCbks31rlVhN5nT0hl+8uOs0zeJ3k\nnmjmlAiDdb6Dt+Jqrj1eMrs4mwSfuFVJjsiB8hW7a+juNu5AvPP70VX7KT/SzDc/MJX0pASa2oOk\nJyWEHUE7jIFd1c2cMS6r12ODSYP+IBqVkcSHZ4/loqmDf8lHEeHDs8eGLfvqxZP46sWT3PvjPV+U\nSQXprPnRgrD1/3BDGcGQ4fWd1W4KaiCumBmew3c88ZVzeWNntduz/sIFExllp5+c9JKTw3R68qkB\nPyLCbR+eERYoclLDx0O8brxwIgUZSW6bnXWccszLzhjDM+sPsr+2NWovKT89iQ9MH8XyrVWMzkp2\nAzVY6Z6PzSvisjPCe50Jfl+v0lLvQK/Dea289CS+fulkrjurmBsWr6Tb4JbcXjKtkD/8c0+vE6x+\nee2cXq93ybTCAacEwepR32MP9Dr56ys8QR/C0z55EUF+fF4qb+y0dsaP3ngOkwvT+a8Xt7s7Vme8\nwtkZTx+TyWs7jrhBvzgnlZV7at0jQkdOaiKBBB8bKxvYUdXMmeNz3e05qcAq6X0xotQyKaGnnU56\npyAjiXnjs3lh82G2HmpkQn4azR1BJhWk88r2akZlJjOnxArQ3nTOB6aP4tYPzWBMdjKJnp1ntP/7\nQIKPf5b3PDc50cd5k/P55NklfOWiSfzhn3t6Bf3Igdrd1S1sPthIp12eW1FnDcj/x5ObCHUb9te2\nct9nymjqsE6Y7Ov7V35Eg/4pJcHv47+vn3vS3i+ylz4+r+eLEk2i30ei30p3HItfXDs76nJnsNnh\nDDoDnD85n18v3+l+0Z3hhPfbAe3i08IDW7IdmCLP1gW44dxS9/bdH5nJB2ZYz51ZlMXzmw6zYEah\ndS3cGFUWv//0PMqrm3v1dEWkz883EOl2OW5eWoBpozOZNtr6DHWtXe5nuWrOWP7wz54pKj55dglX\nRuzAB5NT2eTw9irz08L/NybmWzvnULdxe95pgQS3esdJVzjjR3OKs3hibQVb7CofZ2fgHcQGa7sG\n/D53LKU0L5WFM0bxm+vmcNHUAub/53IeW2Odffzojef02sGeOymf7y06jYumFtDRlcfvXi7nZy9s\n4z8/Yg0QTyhII+D3MTYr2a1oq/H01ItyUqLOd+X8/cdlp5CfHmB2cTZv7aoJm8KkMMOabPBu+70u\nmlrA39dW4PeJW3DhVDmBdSb7I6sOuGdTl+SmUlnfxobKBkLdhtGZySzfeoTqpg6a24OkJydyybRC\nfnD5dC6bOZon11byu1fKCXYbdh4ZvPmJ+qIDuXEkPSmBn18zi0+dXTLUTaGsNJfl37qQT51ltcUJ\ngF9//+So6ztBJlrQ9/rk2T153xsvmMjr372EyYUZ3PqhGfzhc/P7fF6C30pRDba0QO9xHOdIxDkK\nmFWUzcbbF7ozrc4vzeEce1B8MDkllamBBJ656Xxe+vZFgBPgrMCYHRGcJxT0PskwNZBAR7CbYKib\n5zceIiXR7459OGWiztnQzvQUkekdsMYTvnTBBJbcdB5fumAiPp9w1ZxxZKcGOGtCrlsuOWNsZtgg\nOFi9769ePJnkRD9ZqYnccG4pr+6oZredwslPT+LBf5nPFy+YGHUMra+Oj9PDvui0Ap6+6XzuuOoM\nNyXp/O8VRjz3kmmFrL9tIWeO7ymycHZ6S246jzuvPoPUgJ/H7Z3YlbPH0tDWxbf/tg6fwL2fmkuo\n2/D0ukqaO4JkJCWQnOjnSxdOpCgnla9fOoXtd11GaV4qO6uil3oOJg36cebjZcVuj3+oTS7McE/n\n/96i03jmJqsyI5qPziviwqkFfMWTrupPgt/n9uZy0gJh8+OcLOlJvYP+J+2drncMJCM50Q24Tq95\nsP3XNbPZcddlgHUUNMlTRXXTJdZ2jUzvTMpPJ5JzMuETayt4adsRvr1wqpt6mTY6k4DfR/mRZjKS\nE9wdXLTAe8O5pfzgihnMKsruNa3D1XPGubcjx4qiuWBKAcbA85usaq389ADnTsqnICOJ9KSEsEkF\noXfgdrhVWJ72OnM4fXvhVAIJvrATAh0iwgdPH+2mUJ2zfAsykkj0+zhzfI6ddkrj3xdM5fqzSjja\n3MlXLp7EmeNzmVOczeNrKmhq7+rz8542OuOEXo/BoekddVJkJCcys6jvXGVuWoA//ctZJ7FFg8Pp\nzXsHRL94wURmjMnslV64tqyY371S3uc0FMfL5xMCvugD8zecW8qckhxmR/wNxuX03lE6YzR3P7eN\nyYXpfP68nrRdIMHH5MJ0thxqJMtTUus9n2IgPl5WzNjsFBrbuvotJgCYXZxFUoLPPQkvLy18htuc\n1IBbYw+EDcB7RZ6NDNYRyWs7qvn02eNpag+GTXMR2ebLZ47h9NuWuukdpx3nTMzjjZ1HuXBqAX6f\n8JOPznTPVQD42JlF/Ogpa3qQ2UXZvV8c+N3183rtHE8E7ekrdRzS3TLd8J7luZPz3YoXx7cWTOWf\n378kak/yRBMR5hRn9wqwTuWXt37d6ek3tHXxtUsmues4nEH6rBRrR/73r54bdaC/P+dNzueymWP6\nXxFrkPfM8Tnu1BSRFXKRRxp99fSdIx3v+p8/bwIPfv4sfD7ha5dMjjmQnpaUQFrAT2tnyB2sBivv\n7xNYOCP6eNmVs8a66/Z1hv7JCPigQV+p4xItvdMXn0967QiGg7dveT+vf+8S9773PIcPzeo94DzZ\nMw8RwLySnAH11o/XhZ6quMj5/iOrvvrK6U8dlUFeWoDT+7iE6UA4O23ve5wxLos1P1zgnrUeKSs1\n0R28j7w+wMmmQV+p4+D0kI+lBHa4GZOVQp73gkD279lFWWHljg5nrCDyIjMnmjOJHvSuXHN6/h88\nfRTXnFnUq4TUUZSTypofLehzbGkgnLPi8yKO7qLV3nt9zq5Cizx58GTTnL5Sx+EDM0ZxqLHdnSIi\nHpSV5vD+aYVh14D2cnr6R/u5TOJgmxSl0sjhBPmLpha6A+knyncWnsYTayt4/7RR/a/scca4LJbc\ndJ67/YaKBn2ljkNRTiq3XDZ9qJsxqLJTAyyOUf5aak/3Ee1ktRNJxDqBLFp6xOllZ6ac+JBWVprb\n62zigZrVxyDuyaRBXyl1TJIS/Dz9tfP6vALaiXR2H+c35NrlsBnJsc/zUAPM6YvIIhHZLiLlInJz\njPU+JiJGRMrs+6Ui0iYi6+yf3w9Ww5VSQ2d2cXbUE7KGSq49JpExgGtXjHT9biER8QP3AguACmCV\niCwxxmyJWC8D+CbwTsRL7DLG9J5kRCmlBslFUwr48oUTwy6Ko6IbSE//LKDcGLPbGNMJPAJcFWW9\nO4GfAtHnjlVKqRMkKzWRWy6f7paRqr4NZAuNA7yX4amwl7lEZB5QbIx5NsrzJ4jIuyLymohc8N6b\nqpRS6ngddwJMRHzAL4HPRXn4EFBijKkRkTOBp0TkdGNMY8Rr3AjcCFBSMvSThSmlVLwaSE+/Eij2\n3C+ylzkygDOAV0VkL3AOsEREyowxHcaYGgBjzBpgFzA18g2MMfcbY8qMMWUFBYM/F71SSinLQIL+\nKmCKiEwQkQBwHbDEedAY02CMyTfGlBpjSoEVwJXGmNUiUmAPBCMiE4EpwO5B/xRKKaUGpN/0jjEm\nKCI3AUsBP7DYGLNZRO4AVhtjlsR4+oXAHSLSBXQD/2qMqY2xvlJKqRNITIyrDQ2FsrIys3r16qFu\nhlJKnVJEZI0xpqy/9bS+SSmlRhAN+kopNYIMu/SOiFQD+47jJfKBo4PUnJPpVGz3qdhm0HafbNru\nk2O8Mabf8sdhF/SPl4isHkhea7g5Fdt9KrYZtN0nm7Z7eNH0jlJKjSAa9JVSagSJx6B//1A34D06\nFdt9KrYZtN0nm7Z7GIm7nL5SSqm+xWNPXymlVB/iJugP9Opew4GI7BWRjfbVxFbby3JFZJmI7LR/\n5wyDdi4WkSMissmzLGo7xfJbe/tvsKfbHk7tvl1EKj1Xcbvc89gtdru3i8gHh6jNxSLyiohsEZHN\nIvJNe/mw3t4x2j3ct3eyiKwUkfV2u39sL58gIu/Y7XvUnm8MEUmy75fbj5cORbsHhTHmlP/BmhNo\nFzARCADrgRlD3a4Y7d0L5Ecs+xlws337ZuCnw6CdFwLzgE39tRO4HHgeEKyZVt8ZZu2+HfhOlHVn\n2P8vScAE+//IPwRtHgPMs29nADvstg3r7R2j3cN9ewuQbt9OxLri3znA34Dr7OW/B75i3/4q8Hv7\n9nXAo0OxvQfjJ156+gO9utdwdhXwkH37IeDqIWwLAMaY14HICfL6audVwJ+MZQWQLSJjTk5Lw/XR\n7r5cBTxirGnA9wDlWP9PJ5Ux5pAxZq19uwnYinWxomG9vWO0uy/DZXsbY0yzfTfR/jHA+4HH7eWR\n29v5OzwOXCoicpKaO6jiJej3e3WvYcYAL4rIGvsCMgCjjDGH7NuHgVFD07R+9dXOU+FvcJOdClns\nSZ8Nu3bbqYO5WL3PU2Z7R7Qbhvn2FhG/iKwDjgDLsI466o0xwShtc9ttP94A5J3cFg+OeAn6p5rz\njTHzgMuAr4nIhd4HjXUMOezLqk6Vdtr+F5gEzMG6otsvhrY50YlIOvAE8G8m4gpzw3l7R2n3sN/e\nxpiQMWYO1oWhzgKmDXGTTop4Cfr9Xd1rWDHGVNq/jwBPYv3DVTmH5/bvI0PXwpj6auew/hsYY6rs\nL3k38H/0pBSGTbtFJBErcP7FGPN3e/Gw397R2n0qbG+HMaYeeAV4H1aazLnOiLdtbrvtx7OAmpPc\n1EERL0E/5tW9hhMRSRORDOc2sBDYhNXeG+zVbgCeHpoW9quvdi4BPmtXlZwDNHjSEkMuIt/9Eaxt\nDla7r7OrMyZgXd1t5RC0T4A/AFuNMb/0PDSst3df7T4FtneBiGTbt1OABVjjEa8A19irRW5v5+9w\nDfCyfeR16hnqkeTB+sGqZtiBlZf7wVC3J0Y7J2JVL6wHNjttxcoPvgTsBJYDucOgrQ9jHZp3YeU3\nv9BXO7GqIe61t/9GoGyYtfvPdrs2YH2Bx3jW/4Hd7u3AZUPU5vOxUjcbgHX2z+XDfXvHaPdw396z\ngHft9m0CbrWXT8TaCZUDjwFJ9vJk+365/fjEofr/Pt4fPSNXKaVGkHhJ7yillBoADfpKKTWCaNBX\nSqkRRIO+UkqNIBr0lVJqBNGgr5RSI4gGfaWUGkE06Cul1Ajy/wHFB4RrN/3n1gAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120dda6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "\n",
      "EPOCH 0\n",
      "Average loss at step  100  for last 100 steps: 0.692814816833\n",
      "Average loss at step  200  for last 100 steps: 0.668793393373\n",
      "Average loss at step  300  for last 100 steps: 0.647911831737\n",
      "Average loss at step  400  for last 100 steps: 0.639905466437\n",
      "Average loss at step  500  for last 100 steps: 0.629963077307\n",
      "Average loss at step  600  for last 100 steps: 0.612431703806\n",
      "Average loss at step  700  for last 100 steps: 0.604673709869\n",
      "Average loss at step  800  for last 100 steps: 0.600461531878\n",
      "Average loss at step  900  for last 100 steps: 0.586775528193\n",
      "Average loss at step  1000  for last 100 steps: 0.583437932134\n",
      "Average loss at step  1100  for last 100 steps: 0.578215394318\n",
      "Average loss at step  1200  for last 100 steps: 0.562915830016\n",
      "Average loss at step  1300  for last 100 steps: 0.569999979734\n",
      "Average loss at step  1400  for last 100 steps: 0.551043297648\n",
      "Average loss at step  1500  for last 100 steps: 0.560187544227\n",
      "Average loss at step  1600  for last 100 steps: 0.552486325204\n",
      "Average loss at step  1700  for last 100 steps: 0.54977134794\n",
      "Average loss at step  1800  for last 100 steps: 0.551592598557\n",
      "Average loss at step  1900  for last 100 steps: 0.537328742146\n",
      "Average loss at step  2000  for last 100 steps: 0.529305757284\n",
      "Average loss at step  2100  for last 100 steps: 0.536112359762\n",
      "Average loss at step  2200  for last 100 steps: 0.526593842208\n",
      "Average loss at step  2300  for last 100 steps: 0.533741854429\n",
      "Average loss at step  2400  for last 100 steps: 0.528755005598\n",
      "Average loss at step  2500  for last 100 steps: 0.527546128035\n",
      "Average loss at step  2600  for last 100 steps: 0.513521416783\n",
      "Average loss at step  2700  for last 100 steps: 0.513942609429\n",
      "Average loss at step  2800  for last 100 steps: 0.532152716815\n",
      "Average loss at step  2900  for last 100 steps: 0.514523650408\n",
      "Average loss at step  3000  for last 100 steps: 0.516259919703\n",
      "Average loss at step  3100  for last 100 steps: 0.525905211568\n",
      "Average loss at step  3200  for last 100 steps: 0.525074538291\n",
      "Average loss at step  3300  for last 100 steps: 0.521511426866\n",
      "Average loss at step  3400  for last 100 steps: 0.5237329638\n",
      "Average loss at step  3500  for last 100 steps: 0.513138819039\n",
      "Average loss at step  3600  for last 100 steps: 0.512928281724\n",
      "Average loss at step  3700  for last 100 steps: 0.518266055584\n",
      "Average loss at step  3800  for last 100 steps: 0.504069834948\n",
      "Average loss at step  3900  for last 100 steps: 0.497774580121\n",
      "Average loss at step  4000  for last 100 steps: 0.496230741739\n",
      "Average loss at step  4100  for last 100 steps: 0.495737694502\n",
      "Average loss at step  4200  for last 100 steps: 0.502762383223\n",
      "Average loss at step  4300  for last 100 steps: 0.495594077706\n",
      "Average loss at step  4400  for last 100 steps: 0.498948392272\n",
      "Average loss at step  4500  for last 100 steps: 0.494879477322\n",
      "Average loss at step  4600  for last 100 steps: 0.500915962458\n",
      "Average loss at step  4700  for last 100 steps: 0.49238550365\n",
      "Average loss at step  4800  for last 100 steps: 0.498846772313\n",
      "Average loss at step  4900  for last 100 steps: 0.494557002187\n",
      "Average loss at step  5000  for last 100 steps: 0.494088440537\n",
      "Average loss at step  5100  for last 100 steps: 0.48736056149\n",
      "Average loss at step  5200  for last 100 steps: 0.485234865844\n",
      "Average loss at step  5300  for last 100 steps: 0.486547685862\n",
      "Average loss at step  5400  for last 100 steps: 0.488585803807\n",
      "Average loss at step  5500  for last 100 steps: 0.482532167137\n",
      "Average loss at step  5600  for last 100 steps: 0.493242044747\n",
      "Average loss at step  5700  for last 100 steps: 0.480218532383\n",
      "Average loss at step  5800  for last 100 steps: 0.491995439231\n",
      "Average loss at step  5900  for last 100 steps: 0.478645869195\n",
      "Average loss at step  6000  for last 100 steps: 0.498174078763\n",
      "Average loss at step  6100  for last 100 steps: 0.488848202527\n",
      "Average loss at step  6200  for last 100 steps: 0.493301855028\n",
      "Average loss at step  6300  for last 100 steps: 0.512614660561\n",
      "Average loss at step  6400  for last 100 steps: 0.484635056853\n",
      "Average loss at step  6500  for last 100 steps: 0.49781227529\n",
      "Average loss at step  6600  for last 100 steps: 0.50185048461\n",
      "Average loss at step  6700  for last 100 steps: 0.475521313846\n",
      "Average loss at step  6800  for last 100 steps: 0.495895617604\n",
      "Average loss at step  6900  for last 100 steps: 0.493811835349\n",
      "Average loss at step  7000  for last 100 steps: 0.499124832153\n",
      "Average loss at step  7100  for last 100 steps: 0.473645111024\n",
      "Average loss at step  7200  for last 100 steps: 0.504604768157\n",
      "Average loss at step  7300  for last 100 steps: 0.501314915717\n",
      "Average loss at step  7400  for last 100 steps: 0.486240397692\n",
      "Average loss at step  7500  for last 100 steps: 0.489358898997\n",
      "Average loss at step  7600  for last 100 steps: 0.470165685117\n",
      "Average loss at step  7700  for last 100 steps: 0.484775587022\n",
      "Average loss at step  7800  for last 100 steps: 0.495742526948\n",
      "Average loss at step  7900  for last 100 steps: 0.480890091658\n",
      "Average loss at step  8000  for last 100 steps: 0.475974202156\n",
      "Average loss at step  8100  for last 100 steps: 0.491072288752\n",
      "Average loss at step  8200  for last 100 steps: 0.486335908771\n",
      "Average loss at step  8300  for last 100 steps: 0.48413351357\n",
      "Average loss at step  8400  for last 100 steps: 0.486175305843\n",
      "Average loss at step  8500  for last 100 steps: 0.49020080328\n",
      "Average loss at step  8600  for last 100 steps: 0.496255922914\n",
      "Average loss at step  8700  for last 100 steps: 0.489191117585\n",
      "Average loss at step  8800  for last 100 steps: 0.497018876672\n",
      "Average loss at step  8900  for last 100 steps: 0.491091649234\n",
      "Average loss at step  9000  for last 100 steps: 0.479576920867\n",
      "Average loss at step  9100  for last 100 steps: 0.48815962702\n",
      "Average loss at step  9200  for last 100 steps: 0.495990119576\n",
      "Average loss at step  9300  for last 100 steps: 0.486534092724\n",
      "Average loss at step  9400  for last 100 steps: 0.477441419661\n",
      "Average loss at step  9500  for last 100 steps: 0.479804478586\n",
      "Average loss at step  9600  for last 100 steps: 0.481877249181\n",
      "Average loss at step  9700  for last 100 steps: 0.480747143626\n",
      "Average loss at step  9800  for last 100 steps: 0.491088026464\n",
      "Average loss at step  9900  for last 100 steps: 0.486252127886\n",
      "Average loss at step  10000  for last 100 steps: 0.480489980578\n",
      "Average loss at step  10100  for last 100 steps: 0.498533246815\n",
      "Average loss at step  10200  for last 100 steps: 0.485278301835\n",
      "Average loss at step  10300  for last 100 steps: 0.472638519704\n",
      "Average loss at step  10400  for last 100 steps: 0.475944960415\n",
      "Average loss at step  10500  for last 100 steps: 0.485044220388\n",
      "Average loss at step  10600  for last 100 steps: 0.490896600783\n",
      "Average loss at step  10700  for last 100 steps: 0.483900153339\n",
      "Average loss at step  10800  for last 100 steps: 0.487960609794\n",
      "Average loss at step  10900  for last 100 steps: 0.500170060098\n",
      "Average loss at step  11000  for last 100 steps: 0.493307569921\n",
      "Average loss at step  11100  for last 100 steps: 0.478524412662\n",
      "Average loss at step  11200  for last 100 steps: 0.474031868577\n",
      "Average loss at step  11300  for last 100 steps: 0.496644311547\n",
      "Average loss at step  11400  for last 100 steps: 0.476678462923\n",
      "Average loss at step  11500  for last 100 steps: 0.477986085713\n",
      "Average loss at step  11600  for last 100 steps: 0.490213878155\n",
      "Average loss at step  11700  for last 100 steps: 0.495797652006\n",
      "Average loss at step  11800  for last 100 steps: 0.485824933052\n",
      "Average loss at step  11900  for last 100 steps: 0.492229326665\n",
      "Average loss at step  12000  for last 100 steps: 0.46585234195\n",
      "Average loss at step  12100  for last 100 steps: 0.489671885371\n",
      "Average loss at step  12200  for last 100 steps: 0.491961764395\n",
      "Average loss at step  12300  for last 100 steps: 0.496067061126\n",
      "Average loss at step  12400  for last 100 steps: 0.492238401771\n",
      "Average loss at step  12500  for last 100 steps: 0.483120421171\n",
      "Average loss at step  12600  for last 100 steps: 0.487518595755\n",
      "Average loss at step  12700  for last 100 steps: 0.476027235985\n",
      "Average loss at step  12800  for last 100 steps: 0.502071197927\n",
      "Average loss at step  12900  for last 100 steps: 0.489904935658\n",
      "Average loss at step  13000  for last 100 steps: 0.488428077698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  13100  for last 100 steps: 0.485154924393\n",
      "Average loss at step  13200  for last 100 steps: 0.490215238035\n",
      "Average loss at step  13300  for last 100 steps: 0.473665801883\n",
      "Average loss at step  13400  for last 100 steps: 0.483335472047\n",
      "Average loss at step  13500  for last 100 steps: 0.485390028358\n",
      "Average loss at step  13600  for last 100 steps: 0.485867257118\n",
      "Average loss at step  13700  for last 100 steps: 0.49309692204\n",
      "Average loss at step  13800  for last 100 steps: 0.488579553366\n",
      "Average loss at step  13900  for last 100 steps: 0.486183079481\n",
      "Average loss at step  14000  for last 100 steps: 0.493116939366\n",
      "Average loss at step  14100  for last 100 steps: 0.489012809694\n",
      "Average loss at step  14200  for last 100 steps: 0.471927202046\n",
      "Average loss at step  14300  for last 100 steps: 0.474905755818\n",
      "Average loss at step  14400  for last 100 steps: 0.475814082474\n",
      "Average loss at step  14500  for last 100 steps: 0.47002804935\n",
      "Average loss at step  14600  for last 100 steps: 0.469912475646\n",
      "Average loss at step  14700  for last 100 steps: 0.464078573585\n",
      "Average loss at step  14800  for last 100 steps: 0.49291559279\n",
      "Average loss at step  14900  for last 100 steps: 0.488381848335\n",
      "Average loss at step  15000  for last 100 steps: 0.475895235837\n",
      "Average loss at step  15100  for last 100 steps: 0.47332100153\n",
      "Average loss at step  15200  for last 100 steps: 0.476553397775\n",
      "Average loss at step  15300  for last 100 steps: 0.475789139569\n",
      "Average loss at step  15400  for last 100 steps: 0.473593969345\n",
      "Average loss at step  15500  for last 100 steps: 0.486847395897\n",
      "Average loss at step  15600  for last 100 steps: 0.498331933022\n",
      "Average loss at step  15700  for last 100 steps: 0.491534562409\n",
      "Average loss at step  15800  for last 100 steps: 0.483667932749\n",
      "Average loss at step  15900  for last 100 steps: 0.480351713896\n",
      "Average loss at step  16000  for last 100 steps: 0.488517854512\n",
      "Average loss at step  16100  for last 100 steps: 0.480110763311\n",
      "Average loss at step  16200  for last 100 steps: 0.497109397948\n",
      "Average loss at step  16300  for last 100 steps: 0.486281931996\n",
      "Average loss at step  16400  for last 100 steps: 0.4859975034\n",
      "Average loss at step  16500  for last 100 steps: 0.479574407339\n",
      "Average loss at step  16600  for last 100 steps: 0.494770306051\n",
      "Average loss at step  16700  for last 100 steps: 0.479909676313\n",
      "Average loss at step  16800  for last 100 steps: 0.484224578142\n",
      "Average loss at step  16900  for last 100 steps: 0.485350108743\n",
      "Average loss at step  17000  for last 100 steps: 0.474448721409\n",
      "Average loss at step  17100  for last 100 steps: 0.483574254215\n",
      "Average loss at step  17200  for last 100 steps: 0.482149200141\n",
      "Average loss at step  17300  for last 100 steps: 0.487954466045\n",
      "Average loss at step  17400  for last 100 steps: 0.472711369395\n",
      "Average loss at step  17500  for last 100 steps: 0.459172970653\n",
      "Average loss at step  17600  for last 100 steps: 0.485440429449\n",
      "Average loss at step  17700  for last 100 steps: 0.489761842191\n",
      "Average loss at step  17800  for last 100 steps: 0.482554011345\n",
      "Average loss at step  17900  for last 100 steps: 0.482954767942\n",
      "Average loss at step  18000  for last 100 steps: 0.480361146033\n",
      "Average loss at step  18100  for last 100 steps: 0.494280058444\n",
      "Average loss at step  18200  for last 100 steps: 0.472319859564\n",
      "Average loss at step  18300  for last 100 steps: 0.486147353649\n",
      "Average loss at step  18400  for last 100 steps: 0.488846893907\n",
      "Average loss at step  18500  for last 100 steps: 0.48289696157\n",
      "Average loss at step  18600  for last 100 steps: 0.484749906063\n",
      "Average loss at step  18700  for last 100 steps: 0.477763079107\n",
      "Average loss at step  18800  for last 100 steps: 0.490622282922\n",
      "Average loss at step  18900  for last 100 steps: 0.488325515389\n",
      "Average loss at step  19000  for last 100 steps: 0.473045869768\n",
      "Average loss at step  19100  for last 100 steps: 0.473241164088\n",
      "Average loss at step  19200  for last 100 steps: 0.492054203451\n",
      "Average loss at step  19300  for last 100 steps: 0.489754210711\n",
      "Average loss at step  19400  for last 100 steps: 0.480381570458\n",
      "Average loss at step  19500  for last 100 steps: 0.479154531658\n",
      "Average loss at step  19600  for last 100 steps: 0.491381275654\n",
      "Average loss at step  19700  for last 100 steps: 0.486388094425\n",
      "Average loss at step  19800  for last 100 steps: 0.482223565578\n",
      "Average loss at step  19900  for last 100 steps: 0.485705572963\n",
      "Average loss at step  20000  for last 100 steps: 0.490012616217\n",
      "Average loss at step  20100  for last 100 steps: 0.47648139745\n",
      "Average loss at step  20200  for last 100 steps: 0.493204781115\n",
      "Average loss at step  20300  for last 100 steps: 0.48187222749\n",
      "Average loss at step  20400  for last 100 steps: 0.471828006804\n",
      "Average loss at step  20500  for last 100 steps: 0.470677437782\n",
      "Average loss at step  20600  for last 100 steps: 0.482238149345\n",
      "Average loss at step  20700  for last 100 steps: 0.483432168067\n",
      "Average loss at step  20800  for last 100 steps: 0.486470151246\n",
      "Average loss at step  20900  for last 100 steps: 0.48373137176\n",
      "Average loss at step  21000  for last 100 steps: 0.492969449759\n",
      "Average loss at step  21100  for last 100 steps: 0.477727086544\n",
      "Average loss at step  21200  for last 100 steps: 0.480490873456\n",
      "Average loss at step  21300  for last 100 steps: 0.48411534667\n",
      "Average loss at step  21400  for last 100 steps: 0.483604819179\n",
      "Average loss at step  21500  for last 100 steps: 0.471615664363\n",
      "Average loss at step  21600  for last 100 steps: 0.497145136595\n",
      "Average loss at step  21700  for last 100 steps: 0.480350371301\n",
      "Average loss at step  21800  for last 100 steps: 0.489652379453\n",
      "Average loss at step  21900  for last 100 steps: 0.490973730385\n",
      "Average loss at step  22000  for last 100 steps: 0.493135609329\n",
      "Average loss at step  22100  for last 100 steps: 0.486349087358\n",
      "Average loss at step  22200  for last 100 steps: 0.479977960289\n",
      "Average loss at step  22300  for last 100 steps: 0.484179512262\n",
      "Average loss at step  22400  for last 100 steps: 0.489586816728\n",
      "Average loss at step  22500  for last 100 steps: 0.489248315096\n",
      "Average loss at step  22600  for last 100 steps: 0.480545561612\n",
      "Average loss at step  22700  for last 100 steps: 0.479948294163\n",
      "Average loss at step  22800  for last 100 steps: 0.473232050538\n",
      "Average loss at step  22900  for last 100 steps: 0.473489409387\n",
      "Average loss at step  23000  for last 100 steps: 0.489554653466\n",
      "Average loss at step  23100  for last 100 steps: 0.46841783613\n",
      "Average loss at step  23200  for last 100 steps: 0.471685992777\n",
      "Average loss at step  23300  for last 100 steps: 0.497985410988\n",
      "Average loss at step  23400  for last 100 steps: 0.484804165363\n",
      "Average loss at step  23500  for last 100 steps: 0.478986524045\n",
      "Average loss at step  23600  for last 100 steps: 0.476279292703\n",
      "Average loss at step  23700  for last 100 steps: 0.473478781581\n",
      "Average loss at step  23800  for last 100 steps: 0.468336056471\n",
      "Average loss at step  23900  for last 100 steps: 0.469850065708\n",
      "Average loss at step  24000  for last 100 steps: 0.481255723536\n",
      "Average loss at step  24100  for last 100 steps: 0.477461038828\n",
      "Average loss at step  24200  for last 100 steps: 0.479015128314\n",
      "Average loss at step  24300  for last 100 steps: 0.477202915847\n",
      "Average loss at step  24400  for last 100 steps: 0.480947169065\n",
      "Average loss at step  24500  for last 100 steps: 0.479620631933\n",
      "Average loss at step  24600  for last 100 steps: 0.485068547726\n",
      "Average loss at step  24700  for last 100 steps: 0.471502601802\n",
      "Average loss at step  24800  for last 100 steps: 0.467472360134\n",
      "Average loss at step  24900  for last 100 steps: 0.476847848892\n",
      "Average loss at step  25000  for last 100 steps: 0.491403426826\n",
      "Average loss at step  25100  for last 100 steps: 0.486768456995\n",
      "Average loss at step  25200  for last 100 steps: 0.476867712736\n",
      "Average loss at step  25300  for last 100 steps: 0.472427942753\n",
      "Average loss at step  25400  for last 100 steps: 0.478575013578\n",
      "Average loss at step  25500  for last 100 steps: 0.47319616437\n",
      "Average loss at step  25600  for last 100 steps: 0.473089240193\n",
      "Average loss at step  25700  for last 100 steps: 0.48891089648\n",
      "Average loss at step  25800  for last 100 steps: 0.472889746726\n",
      "Average loss at step  25900  for last 100 steps: 0.493005713522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  26000  for last 100 steps: 0.486762501895\n",
      "Average loss at step  26100  for last 100 steps: 0.475316224396\n",
      "Average loss at step  26200  for last 100 steps: 0.473075286746\n",
      "Average loss at step  26300  for last 100 steps: 0.477880786657\n",
      "Average loss at step  26400  for last 100 steps: 0.481963812709\n",
      "Average loss at step  26500  for last 100 steps: 0.472947643399\n",
      "Average loss at step  26600  for last 100 steps: 0.479513393342\n",
      "Average loss at step  26700  for last 100 steps: 0.478903036118\n",
      "Average loss at step  26800  for last 100 steps: 0.474650253952\n",
      "Average loss at step  26900  for last 100 steps: 0.458804207444\n",
      "Average loss at step  27000  for last 100 steps: 0.467803621292\n",
      "Average loss at step  27100  for last 100 steps: 0.475298628807\n",
      "Average loss at step  27200  for last 100 steps: 0.483436170816\n",
      "Average loss at step  27300  for last 100 steps: 0.475533187091\n",
      "Average loss at step  27400  for last 100 steps: 0.475883714259\n",
      "Average loss at step  27500  for last 100 steps: 0.484315147102\n",
      "Average loss at step  27600  for last 100 steps: 0.470449980497\n",
      "Average loss at step  27700  for last 100 steps: 0.472445224226\n",
      "Average loss at step  27800  for last 100 steps: 0.477320248783\n",
      "Average loss at step  27900  for last 100 steps: 0.484965956807\n",
      "Average loss at step  28000  for last 100 steps: 0.47936653316\n",
      "Average loss at step  28100  for last 100 steps: 0.494527530074\n",
      "Average loss at step  28200  for last 100 steps: 0.484037095308\n",
      "Average loss at step  28300  for last 100 steps: 0.464017668664\n",
      "Average loss at step  28400  for last 100 steps: 0.48092647016\n",
      "Average loss at step  28500  for last 100 steps: 0.474472622871\n",
      "Average loss at step  28600  for last 100 steps: 0.475558381677\n",
      "Average loss at step  28700  for last 100 steps: 0.47563845396\n",
      "Average loss at step  28800  for last 100 steps: 0.477217969298\n",
      "Average loss at step  28900  for last 100 steps: 0.470290863514\n",
      "Average loss at step  29000  for last 100 steps: 0.474169459045\n",
      "Average loss at step  29100  for last 100 steps: 0.478585951626\n",
      "Average loss at step  29200  for last 100 steps: 0.477009248137\n",
      "Average loss at step  29300  for last 100 steps: 0.475781868398\n",
      "Average loss at step  29400  for last 100 steps: 0.492932217121\n",
      "Average loss at step  29500  for last 100 steps: 0.475711449087\n",
      "Average loss at step  29600  for last 100 steps: 0.490468889773\n",
      "Average loss at step  29700  for last 100 steps: 0.476737249792\n",
      "Average loss at step  29800  for last 100 steps: 0.473842723668\n",
      "Average loss at step  29900  for last 100 steps: 0.463307519555\n",
      "Average loss at step  30000  for last 100 steps: 0.47838049233\n",
      "Average loss at step  30100  for last 100 steps: 0.470937519372\n",
      "Average loss at step  30200  for last 100 steps: 0.474559289515\n",
      "Average loss at step  30300  for last 100 steps: 0.469879240692\n",
      "Average loss at step  30400  for last 100 steps: 0.480325071812\n",
      "Average loss at step  30500  for last 100 steps: 0.481573143899\n",
      "Average loss at step  30600  for last 100 steps: 0.477850035429\n",
      "Average loss at step  30700  for last 100 steps: 0.479558460712\n",
      "Average loss at step  30800  for last 100 steps: 0.473324997127\n",
      "Average loss at step  30900  for last 100 steps: 0.487738307416\n",
      "Average loss at step  31000  for last 100 steps: 0.470141828358\n",
      "Average loss at step  31100  for last 100 steps: 0.476172268689\n",
      "Average loss at step  31200  for last 100 steps: 0.476369267702\n",
      "Average loss at step  31300  for last 100 steps: 0.478661800623\n",
      "Average loss at step  31400  for last 100 steps: 0.486210770011\n",
      "Average loss at step  31500  for last 100 steps: 0.473322082758\n",
      "Average loss at step  31600  for last 100 steps: 0.466060412824\n",
      "Average loss at step  31700  for last 100 steps: 0.478616007864\n",
      "Average loss at step  31800  for last 100 steps: 0.491355279684\n",
      "Average loss at step  31900  for last 100 steps: 0.474697775245\n",
      "Average loss at step  32000  for last 100 steps: 0.475329670012\n",
      "Average loss at step  32100  for last 100 steps: 0.48772515744\n",
      "Average loss at step  32200  for last 100 steps: 0.479631394446\n",
      "Average loss at step  32300  for last 100 steps: 0.469802727401\n",
      "Average loss at step  32400  for last 100 steps: 0.475892446935\n",
      "Average loss at step  32500  for last 100 steps: 0.479046375751\n",
      "Average loss at step  32600  for last 100 steps: 0.480081523955\n",
      "Average loss at step  32700  for last 100 steps: 0.472463859022\n",
      "Average loss at step  32800  for last 100 steps: 0.478304235935\n",
      "Average loss at step  32900  for last 100 steps: 0.479391312897\n",
      "Average loss at step  33000  for last 100 steps: 0.467144238949\n",
      "Average loss at step  33100  for last 100 steps: 0.471613267958\n",
      "Average loss at step  33200  for last 100 steps: 0.477681069374\n",
      "Average loss at step  33300  for last 100 steps: 0.468515498042\n"
     ]
    }
   ],
   "source": [
    "training_losses = train_network(1, second_idx = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEHlEgr4QQuSR/x/51lMcL62KuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e75b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
